<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Xi Shen</title>
  
  <meta name="author" content="Xi Shen">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <style>
    #more {display: none;}
  </style>
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/web_icon.jpeg">
<script async defer src="https://buttons.github.io/buttons.js"></script>
<script src="https://ajax.googleapis.com/ajax/libs/jquery/2.1.1/jquery.min.js"></script>
</head>

<script>
function LoadMore() {
  var dots = document.getElementById("dots");
  var moreText = document.getElementById("more");
  var btnText = document.getElementById("myBtn");

  if (dots.style.display === "none") {
    dots.style.display = "inline";
    btnText.innerHTML = "SHOW MORE..."; 
    moreText.style.display = "none";
  } else {
    dots.style.display = "none";
    btnText.innerHTML = "HIDE..."; 
    moreText.style.display = "inline";
  }
}
</script>



<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Xi Shen</name> 
              </p>

              <p style="text-align:center">
                Email: <i><u>shenxiluc at gmail dot com</u></i> 
              </p>
              
              <p>
                <b>Bio: </b>
                I hold the position of Chief Scientist at <a href="https://www.intellindust.com/">Intellindust</a>. My team aims at building reliable and robust computer vision algorimths on edge devices with limited NPU power.
              </p>
              
              <p>
                I completed my Ph.D. within the <a href="http://imagine.enpc.fr/">IMAGINE team</a>, <a href="http://www.enpc.fr/">École des Ponts ParisTech</a> under the supervision of <a href="http://imagine.enpc.fr/~aubrym/">Prof. Mathieu Aubry</a>. I also collaborate closely with <a href="https://people.eecs.berkeley.edu/~efros/"> Prof. Alexei A. Efros</a> from UC Berkeley. Previously, I was a Senior Researcher at  <a href="https://ai.tencent.com">Tencent AI Lab</a>, Shenzhen, China.
              </p>
              <p>
              I completed the Master <a href="http://math.ens-paris-saclay.fr/version-francaise/formations/master-mva/">Vision et Apprentissage</a> program at ENS Paris-Saclay and the Engineering Program at <a href="http://www.enpc.fr/">Ecole des Ponts ParisTech</a>. Prior to that, I received dual B.S. degrees in Mechanics from <a href="https://www.univ-lyon1.fr/">Université Claude Bernard Lyon</a> and Physics from <a href="https://en.whu.edu.cn/">Wuhan University</a>. 
              </p>

              <p>
              If the internship opportunity focusing on safety AI and efficient training catches your attention, don't hesitate to reach out via email. Relevant areas of interest may include self-supervised representation learning, low-shot learning, coreset selection, uncertainty estimation, and real-time low-light video enhancement.
              </p>

              <p style="text-align:center">
              <div id="circles_container">
                  <a href='http://github.com/XiSHEN0220'><img class="iconImg" src="images/github.png" alt="Github">GitHub</a>
                  <a href='Xi_Shen_CV.pdf'><img class="iconImg" src="images/cv.png" alt="CV">CV</a>
                  <a href='https://scholar.google.com/citations?user=nKSXus4AAAAJ&hl=en'><img class="iconImg" src="images/googleScholar.png" alt="Google Scholar">Google Scholar</a>
                  
                </div>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/xishen.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/xishen.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
  
  <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
  <tr><td style="padding:20px;width:100%;vertical-align:middle">
              <heading>News</heading>
  <hr/>
    
        <ul>
        <li>
          <b style="background-image: linear-gradient(to top, #f3e7e9 0%, #e3eeff 99%, #e3eeff 100%);">27 / 02 / 2024</b>&emsp;
           2/2 papers are accepted by CVPR 2024, thanks to all my co-authors!
        </li>
          
        <li>
          <b style="background-image: linear-gradient(to top, #f3e7e9 0%, #e3eeff 99%, #e3eeff 100%);">01 / 08 / 2023</b>&emsp;
           <a href="https://www.m-psi.fr/Papers/TokenCut2022/">"TokenCut"</a> is accepted by TPAMI.
        </li>
        <li>
          <b style="background-image: linear-gradient(to top, #f3e7e9 0%, #e3eeff 99%, #e3eeff 100%);">01 / 07 / 2023</b>&emsp;
           <a href="https://arxiv.org/abs/2309.09294">"LivelySpeaker"</a> is accepted by ICCV 2023.
        </li>
        <li>
          <b style="background-image: linear-gradient(to top, #f3e7e9 0%, #e3eeff 99%, #e3eeff 100%);">01 / 03 / 2023</b>&emsp;
           3/3 papers are accepted by CVPR 2023, thanks to all my co-authors!
        </li>  
        <li>
          <b style="background-image: linear-gradient(to top, #f3e7e9 0%, #e3eeff 99%, #e3eeff 100%);">15 / 01 / 2023</b>&emsp;
           Our paper <a href="https://mael-zys.github.io/T2M-GPT/">T2M-GPT: Generating Human Motion from Textual Descriptions with Discrete Representations</a> is available on arXiv.
        </li>
        <li>
          <b style="background-image: linear-gradient(to top, #f3e7e9 0%, #e3eeff 99%, #e3eeff 100%);">22 / 11 / 2022</b>&emsp;
           Our paper <a href="https://sadtalker.github.io/">SadTalker: Learning Realistic 3D Motion Coefficients for Stylized Audio-Driven Single Image Talking Face Animation</a> is available on arXiv.
        </li>
        
        <li>
          <b style="background-image: linear-gradient(to top, #f3e7e9 0%, #e3eeff 99%, #e3eeff 100%);">30 / 08 / 2022</b>&emsp;
           Our paper <a href="https://arxiv.org/abs/2207.01567">Back to MLP: A Simple Baseline for Human Motion Prediction (siMLPe)</a> is accepted to WACV 2023.
        </li>

        <li>
          <b style="background-image: linear-gradient(to top, #f3e7e9 0%, #e3eeff 99%, #e3eeff 100%);">14 / 07 / 2022</b>&emsp;
           I am honoured to be acknowledged as Outstanding Reviewer for ICML 2022.
        </li>
        
        <li>
          <b style="background-image: linear-gradient(to top, #f3e7e9 0%, #e3eeff 99%, #e3eeff 100%);">27 / 06 / 2022</b>&emsp;
           Our paper <a href="https://yingyichen-cyy.github.io/CompressFeatNoisyLabels/">Compressing Features for Learning with Noisy Labels</a> is accepted to TNNLS 2022.
        </li>
        <li>
          <b style="background-image: linear-gradient(to top, #f3e7e9 0%, #e3eeff 99%, #e3eeff 100%);">19 / 06 / 2022</b>&emsp;
           CVPR 2022 workshop presentations: <a href="https://www.m-psi.fr/Papers/TokenCut2022/">"TokenCut"</a> in <a href="https://sites.google.com/view/l3d-ivu/">"L3D"</a>;  <a href="http://imagine.enpc.fr/~shenx/SegSwap/">"SegSwap"</a> in <a href="https://image-matching-workshop.github.io/">"Image Matching"</a> and <a href="https://sites.google.com/view/t4v-cvpr22">"Transformer"</a>.
        </li>  
          
        <li>
          <b style="background-image: linear-gradient(to top, #f3e7e9 0%, #e3eeff 99%, #e3eeff 100%);">02 / 03 / 2022</b>&emsp;
           <a href="https://www.m-psi.fr/Papers/TokenCut2022/">"TokenCut"</a> is accepted to CVPR 2022. Check <a href="https://huggingface.co/spaces/akhaliq/TokenCut">Gradio Demo</a> here. 
        </li>  
        <li>
          <b style="background-image: linear-gradient(to top, #f3e7e9 0%, #e3eeff 99%, #e3eeff 100%);">25 / 02 / 2022</b>&emsp;
          Our paper <a href="https://www.m-psi.fr/Papers/TokenCut2022/">"Self-Supervised Transformers for Unsupervised Object Discovery using Normalized Cut"</a> (TokenCut) is published on arXiv.
        </li>
        <li>
          <b style="background-image: linear-gradient(to top, #f3e7e9 0%, #e3eeff 99%, #e3eeff 100%);">13 / 01 / 2022</b>&emsp;
          Our paper <a href="http://imagine.enpc.fr/~shenx/HisImgAnalysis/">"Spatially-consistent Feature Matching and Learning for Heritage Image Analysis"</a> is accepted to <a href="https://nips.cc/">IJCV</a>.
        </li>
        <span id="dots"> </span>
        <span id="more">
        <li>
          <b style="background-image: linear-gradient(to top, #f3e7e9 0%, #e3eeff 99%, #e3eeff 100%);">28 / 12 / 2021</b>&emsp;
          I join <a href="https://ai.tencent.com">Tencent AI Lab</a> as a Senior Researcher, I will work with <a href="https://juew.org/">Dr. Jue Wang</a> on Computer Vision.
        </li>
        <li>
          <b style="background-image: linear-gradient(to top, #f3e7e9 0%, #e3eeff 99%, #e3eeff 100%);">03 / 12 / 2021</b>&emsp;
          I am defending my Ph.D. today! <a href="Xi_Shen_defense_slides.pdf">[Slides]</a> <a href="Xi_Shen_thesis.pdf">[Thesis]</a>
        </li>
        <li>
          <b style="background-color:#ddd;">29 / 10 / 2021</b>&emsp;
          Our paper <a href="http://imagine.enpc.fr/~shenx/SegSwap/">"Learning Co-segmentation by Segment Swapping for Retrieval and Discovery"</a> is published on arXiv.
        </li> 
        <li>
          <b style="background-image: linear-gradient(to top, #f3e7e9 0%, #e3eeff 99%, #e3eeff 100%);">29 / 09 / 2021</b>&emsp;
          Our paper <a href="http://imagine.enpc.fr/~shenx/SSR/">"Re-ranking for image retrieval and transductive few-shot classification"</a> is accepted to <a href="https://nips.cc/">Neurips 2021</a>.
        </li>
        
          <li>
          <b style="background-image: linear-gradient(to top, #f3e7e9 0%, #e3eeff 99%, #e3eeff 100%);">18 / 04 / 2021</b>&emsp;
          Our paper "<a href="https://imagine.enpc.fr/~shenx/ImageCollation/"> Image Collation: Matching illustrations in manuscripts</a>" is accepted to <a href="https://icdar2021.org/">ICDAR 2021</a>.
        </li>
          
          <li>
          <b style="background-image: linear-gradient(to top, #f3e7e9 0%, #e3eeff 99%, #e3eeff 100%);">15 / 07 / 2020</b>&emsp;
          Our paper "<a href="http://imagine.enpc.fr/~shenx/RANSAC-Flow/"> RANSAC-Flow: generic two-stage image alignment</a>" is accepted to <a href="https://eccv2020.eu/">ECCV 2020</a>. Watch the <a href="http://imagine.enpc.fr/~shenx/RANSAC-Flow/img/demo_ransac_flow.mp4"> demo</a>.
        </li>
        <li>
          <b style="background-color:#ddd;">07 / 03 / 2020</b>&emsp;
          The web application of historical watermark recognition is online!! Check the <a href="https://hal.inria.fr/hal-02513038/document"> paper </a> and the  <a href="https://filigranes.inria.fr/#/filigrane-search"> web application </a>.
        </li> 
        <li>
          <b style="background-color:#ddd;">12 / 12 / 2019</b>&emsp;
          Our paper "<a href="https://openreview.net/forum?id=Hkg-xgrYvH"> Empirical Bayes Transductive Meta-Learning with Synthetic Gradients</a>" accepted to <a href="https://iclr.cc/Conferences/2020">ICLR 2020</a>.
        </li> 
        <li>
          <b style="background-color:#ddd;">06 / 08 / 2019</b>&emsp;
          Our paper "<a href="http://imagine.enpc.fr/~shenx/Watermark"> Large-Scale Historical Watermark Recognition: dataset and a new consistency-based approach</a>" is published on arXiv.
        </li> 
        
        </span>
        <div style="text-align:center">
        <button class="button-2" role="button" onclick="LoadMore()" id="myBtn" >SHOW MORE...</button>
      </div>
        </ul>
  </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <hr/>
              <p>
                
                <sup>*</sup> indicates equal contribution. </br>

                <sup>+</sup> indicates corresponding author. </br>

                Representative papers are <span class="highlight">highlighted</span>.
              </p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div style="display: flex; justify-content: center;">
                <img src='images/2022_tokencut_video.gif' width="160" >
              </div>
              
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://www.m-psi.fr/Papers/TokenCut2022/">
                <papertitle>TokenCut: Segmenting Objects in Images and Videos with Self-supervised Transformer and Normalized Cut</papertitle>
              </a>
              
              </br>
              
              <a href="https://yangtaowang95.github.io/">Yangtao Wang</a>,
              <strong>Xi Shen<sup>+</sup></strong>,
              <a href="https://yangtaowang95.github.io/">Yuan Yuan</a>,
              <a href="https://dulucas.github.io/">Yuming Du</a>,
              <a href="https://scholar.google.com/citations?user=ym_t6QYAAAAJ&hl=zh-CN">Maomao Li</a>,
              <a href="http://hushell.github.io">Xu Hu</a>, 
              <a href="http://crowley-coutaz.fr/jlc/jlc.html">James Crowley</a>,
              <a href="https://research.vaufreydaz.org/">Dominique Vaufreydaz</a>
              </br>
              <em>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</em>, 2023
              </br>
              <a href="https://www.m-psi.fr/Papers/TokenCut2022/">[Project page]</a> &nbsp;
              <a href="https://arxiv.org/abs/2209.00383">[PDF]</a> &nbsp;
              <a href="https://github.com/YangtaoWANG95/TokenCut_video">[Code (GitHub)]</a> &nbsp;
              <a class="github-button" href="https://github.com/YangtaoWANG95/TokenCut_video" data-icon="octicon-star" data-size="medium" data-show-count="true">Star</a> 
            </td>
            </tr>

            <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div style="display: flex; justify-content: center;">
                <img src='images/livelyspeaker.gif' width="160" >
              </div>
              
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://github.com/zyhbili/LivelySpeaker">
                <papertitle>LivelySpeaker: Towards Semantic-aware Co-speech Gesture Generation</papertitle>
              </a>
              
              </br>
              
              <a>Yihao Zhi<sup>*</sup></a>,
              <a href="https://vinthony.github.io/academic/">Xiaodong Cun<sup>*</sup></a>,
              <a href="https://xuelin-chen.github.io/">Xuelin Chen</a>,
              <b>Xi Shen</b>,
              <a href="https://guo-w.github.io//">Wen Guo</a>,
              <a>Shaoli Huang</a>,
              <a href="https://svip-lab.github.io/team.html">Shenghua Gao</a>
              </br>
              <em>ICCV</em>, 2023
              </br>
              <a href="https://github.com/zyhbili/LivelySpeaker">[Project page]</a> &nbsp;
              <a href="https://arxiv.org/abs/2309.09294">[PDF]</a> &nbsp;
              <a href="https://github.com/zyhbili/LivelySpeaker">[Code (GitHub)]</a> &nbsp;
              <a class="github-button" href="https://github.com/zyhbili/LivelySpeaker" data-icon="octicon-star" data-size="medium" data-show-count="true">Star</a> 
            </td>
            </tr>

            <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div style="display: flex; justify-content: center;">
                <img src='images/vpt.png' width="160" >
              </div>
              
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://nifangbaage.github.io/Explicit-Visual-Prompt/">
                <papertitle>Explicit Visual Prompting for Low-Level Structure Segmentations</papertitle>
              </a>
              </br>
              
              Weihuang Liu,
              <strong>Xi Shen</strong>,
              <a href="https://www.cis.um.edu.mo/~cmpun/">Chi-Man Pun<sup>+</sup></a>,
              <a href="https://vinthony.github.io/academic/">Xiaodong Cun<sup>+</sup></a>
              
              </br>
              <em>CVPR</em>, 2023
              </br>
              <a href="https://nifangbaage.github.io/Explicit-Visual-Prompt/">[Project page]</a> &nbsp;
              <a href="https://arxiv.org/pdf/2303.10883.pdf">[PDF]</a> &nbsp;
              <a href="https://github.com/NiFangBaAGe/Explicit-Visual-Prompt">[Code (GitHub)]</a>
              <a class="github-button" href="https://github.com/NiFangBaAGe/Explicit-Visual-Prompt" data-icon="octicon-star" data-size="medium" data-show-count="true">Star</a> 
              

            </td>
            </tr>
            <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div style="display: flex; justify-content: center;">
                <img src='images/t2m.gif' width="160" >
              </div>
              
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://mael-zys.github.io/T2M-GPT/">
                <papertitle>T2M-GPT: Generating Human Motion from Textual Descriptions with Discrete Representations</papertitle>
              </a>
              </br>
              
              Jianrong Zhang<sup>*</sup>,
              Yangsong Zhang<sup>*</sup>, 
              
              <a href="https://vinthony.github.io/academic/">Xiaodong Cun</a>,
              <a href="https://scholar.google.com/citations?user=o31BPFsAAAAJ&hl=en">Shaoli Huang</a>,
              <a href="https://yzhang2016.github.io/">Yong Zhang</a>,
              Hongwei Zhao,
              Hongtao Lu,
              <strong>Xi Shen<sup>+</sup></strong>
              </br>
              <em>CVPR</em>, 2023
              </br>
              <a href="https://mael-zys.github.io/T2M-GPT/">[Project page]</a> &nbsp;
              <a href="hhttps://arxiv.org/abs/2301.06052">[PDF]</a> &nbsp;
              <a href="https://github.com/Mael-zys/T2M-GPT">[Code (GitHub)]</a> &nbsp;
              <a href="https://colab.research.google.com/drive/1Vy69w2q2d-Hg19F-KibqG0FRdpSj3L4O?usp=sharing">[Online demo (Colab)]</a> &nbsp;
              <a class="github-button" href="https://github.com/Mael-zys/T2M-GPT" data-icon="octicon-star" data-size="medium" data-show-count="true">Star</a> 
              
              

            </td>
            </tr>
            <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div style="display: flex; justify-content: center;">
                <img src='images/sadtalker.gif' width="160" >
              </div>
              
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://sadtalker.github.io/">
                <papertitle>SadTalker: Learning Realistic 3D Motion Coefficients for Stylized Audio-Driven Single Image Talking Face Animation</papertitle>
              </a>
              </br>
              
              Wenxuan Zhang<sup>*</sup>,
              <a href="https://vinthony.github.io/academic/">Xiaodong Cun</a><sup>*</sup>,
              
              <a href="https://xuanwangvc.github.io/">Xuan Wang</a>,
              <a href="https://yzhang2016.github.io/">Yong Zhang</a>,
              <strong>Xi Shen</strong>,
              Guo Yu,
              Ying Shan,
              Fei Wang
              </br>
              <em>CVPR</em>, 2023
              </br>
              <a href="https://medium.com/voxel51/cvpr-2023-survival-guide-504e965e1f8b">
              Top 10 you won't miss papers of CVPR 2023 (Voxel51).</a>
              </br>
              <a href="https://sadtalker.github.io/">[Project page]</a> &nbsp;
              <a href="https://arxiv.org/pdf/2211.12194.pdf">[PDF]</a> &nbsp;
              <a href="https://github.com/Winfredy/SadTalker">[Code (GitHub)]</a> &nbsp;
              <a class="github-button" href="https://github.com/Winfredy/SadTalker" data-icon="octicon-star" data-size="medium" data-show-count="true">Star</a> 

            </td>
            </tr>
            <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div style="display: flex; justify-content: center;">
                <img src='images/simple.png' width="160" >
              </div>
              
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://github.com/dulucas/siMLPe">
                <papertitle>Back to MLP: A Simple Baseline for Human Motion Prediction (siMLPe)</papertitle>
              </a>
              </br>
              
              <a href="https://guo-w.github.io//">Wen Guo<sup>*</sup></a>,
              <a href="https://dulucas.github.io/Homepage/">Yuming Du<sup>*</sup></a>, 
              <strong>Xi Shen<sup>+</sup></strong>,
              <a href="https://vincentlepetit.github.io/">Vincent Lepetit</a>,
              <a href="http://xavirema.eu/">Xavier Alameda-Pineda</a>,
              <a href="https://www.iri.upc.edu/people/fmoreno/">Francesc Moreno-Noguer</a>
              
              </br>
              <em>WACV</em>, 2023
              </br>
              <a href="https://github.com/dulucas/siMLPe">[Project page]</a> &nbsp;
              <a href="https://arxiv.org/abs/2207.01567">[PDF]</a> &nbsp;
              <a href="https://github.com/dulucas/siMLPe">[siMLPe Code (GitHub)]</a> &nbsp;
              <a class="github-button" href="https://github.com/dulucas/siMLPe" data-icon="octicon-star" data-size="medium" data-show-count="true">Star</a>  
              

            </td>
            </tr>


            <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div style="display: flex; justify-content: center;">
                <img src='images/sortout.jpeg' width="160" >
              </div>
              
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://yingyichen-cyy.github.io/CompressFeatNoisyLabels/">
                <papertitle>Compressing Features for Learning with Noisy Labels</papertitle>
              </a>
              </br>
              
              <a href="https://github.com/yingyichen-cyy/">Yingyi Chen</a>,
              <a href="http://hushell.github.io">Xu Hu</a>, 
              <strong>Xi Shen<sup>+</sup></strong>,
              <a href="https://myweb.cuhk.edu.cn/chunrongai">Chunrong Ai</a>,
              <a href="https://www.esat.kuleuven.be/sista/members/suykens.html">Johan A.K. Suykens</a>
              </br>
              <em>TNNLS</em>, 2022
              </br>
              <a href="https://yingyichen-cyy.github.io/CompressFeatNoisyLabels/">[Project page]</a> &nbsp;
              <a href="https://yingyichen-cyy.github.io/CompressFeatNoisyLabels/">[PDF]</a> &nbsp;
              <a href="https://github.com/yingyichen-cyy/Nested-Co-teaching">[Code (GitHub)]</a> &nbsp;
              <a href="https://www.youtube.com/watch?v=y9zBDioKMM0&t=5s">[Workshop Video (7 mins)]</a> &nbsp;
              <a class="github-button" href="https://github.com/yingyichen-cyy/Nested-Co-teaching" data-icon="octicon-star" data-size="medium" data-show-count="true">Star</a>  
              

            </td>
            </tr>
            
            <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div style="display: flex; justify-content: center;">
                <img src='images/tokencut.gif' width="160" >
              </div>
              
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://www.m-psi.fr/Papers/TokenCut2022/">
                <papertitle>Self-Supervised Transformers for Unsupervised Object Discovery using Normalized Cut</papertitle>
              </a>
              </br>
              
              <a href="https://yangtaowang95.github.io/">Yangtao Wang</a>,
              <strong>Xi Shen<sup>+</sup></strong>,
              <a href="http://hushell.github.io">Xu Hu</a>, 
                <a href="https://yyuanad.github.io">Yuan Yuan</a>,
                <a href="http://crowley-coutaz.fr/jlc/jlc.html">James Crowley</a>,
                <a href="https://research.vaufreydaz.org/">Dominique Vaufreydaz</a>
              </br>
              <em>CVPR</em>, 2022
              </br>
              <a href="https://www.m-psi.fr/Papers/TokenCut2022/">[Project page]</a> &nbsp;
              <a href="https://arxiv.org/pdf/2202.11539.pdf">[PDF]</a> &nbsp;
              <a href="https://github.com/YangtaoWANG95/TokenCut">[Code (GitHub)]</a> &nbsp;
              <a href="https://colab.research.google.com/github/YangtaoWANG95/TokenCut/blob/master/inference_demo.ipynb">[Colab]</a> &nbsp;
              <a href="https://huggingface.co/spaces/akhaliq/TokenCut">[Gradio Demo]</a> &nbsp;
              <a href="https://gricad-gitlab.univ-grenoble-alpes.fr/wangyan/tokencut">[Code (GitLab)]</a> &nbsp;
              <a class="github-button" href="https://github.com/YangtaoWANG95/TokenCut" data-icon="octicon-star" data-size="medium" data-show-count="true">Star</a>  
              

            </td>
            </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:center">
              <div style="display: flex; justify-content: center;">
                <img src='images/ijcv2022.jpg' width="160">
              </div>
              
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="http://imagine.enpc.fr/~shenx/HisImgAnalysis/">
                <papertitle>Spatially-consistent Feature Matching and Learning for Heritage Image Analysis</papertitle>
              </a>
              </br>

              
              <strong>Xi Shen</strong>,
              <a href="https://robin-champenois.fr/">Robin Champenois</a>,
              <a href="https://people.eecs.berkeley.edu/~shiry/"> Shiry Ginosar</a>,
              <a href="http://www.chartes.psl.eu/en/ilaria-pastrolin">Ilaria Pastrolin</a>,
              <a href="https://www.ademec.com/en/association/members/morgane-rousselot">Morgane Rousselot</a>,
              <a href="https://scholar.google.com/citations?user=B2RS1M4AAAAJ&hl=en">Oumayma Bounou</a>,
              <a href="https://imagine.enpc.fr/~monniert/">Tom Monnier</a>,
              <a href="https://scholar.google.fr/citations?user=7atfg7EAAAAJ&hl=en">Spyros Gidaris</a>,
              <a href="https://www.irht.cnrs.fr/fr/annuaire/bougard-francois">Fran&ccedil;ois Bougard</a>,
              <a href="https://www.rocq.inria.fr/arles/index.php/software/106-pierre-guillaume-raverdy">Pierre-Guillaume Raverdy</a>,
              <a href="http://www.chartes.psl.eu/fr/marie-francoise-limon-bonnet">Marie-Fran&ccedil;oise Limon</a>,
              <a href="http://www.chartes.psl.eu/fr/christine-benevent">Christine B&eacute;n&eacute;vent</a>,
              <a href="http://www.chartes.psl.eu/fr/marc-smith">Marc Smith</a>,
              <a href="http://www.chartes.psl.eu/fr/olivier-poncet">Olivier Poncet</a>,
              <a href="https://scholar.google.be/citations?user=gW9pIqEAAAAJ&hl=en">K. Bender</a>,
              <a href="https://www.unige.ch/lettres/humanites-numeriques/fr/equipe/collaborateurs/prof-beatrice-joyeux-prunel/">B&eacute;atrice Joyeux-Prunel</a>,
              <a href="https://arthistory.berkeley.edu/people/elizabeth-honig/"> Elizabeth Honig</a>,
              <a href="https://people.eecs.berkeley.edu/~efros/"> Alexei A. Efros</a>,
            <a href="http://imagine.enpc.fr/~aubrym/">Mathieu Aubry</a>
              </br>
              <em>IJCV</em>, 2022
              </br>
              <a href="http://imagine.enpc.fr/~shenx/HisImgAnalysis/">[Project page]</a> &nbsp;
              <a href="https://link.springer.com/article/10.1007/s11263-022-01576-x">[PDF (Springer)]</a>
              &nbsp;
              <a href="https://hal.archives-ouvertes.fr/hal-03620996">[PDF (HAL)]</a>
            </td>
        </tr>


        <tr bgcolor="#ffffd0">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div style="display: flex; justify-content: center;">
                <img src='images/teaser_SegSwap.jpeg' width="160" >
              </div>
              
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="http://imagine.enpc.fr/~shenx/SegSwap/">
                <papertitle>Learning Co-segmentation by Segment Swapping for Retrieval and Discovery</papertitle>
              </a>
              </br>
              
              <strong>Xi Shen</strong>,
              <a href="https://people.eecs.berkeley.edu/~efros/"> Alexei A. Efros</a>,
              <a href="https://ai.facebook.com/people/armand-joulin/">Armand Joulin</a>,
              <a href="http://imagine.enpc.fr/~aubrym/">Mathieu Aubry</a>
              </br>
              <em>CVPR </em> <a href="https://image-matching-workshop.github.io/">Image Matching</a> workshop and <a href="https://sites.google.com/view/t4v-cvpr22">Transformer</a> workshop, 2022
              </br>
              <a href="http://imagine.enpc.fr/~shenx/SegSwap/">[Project page]</a> &nbsp;
              <a href="http://arxiv.org/abs/2110.15904">[PDF]</a> &nbsp;
              <a href="https://github.com/XiSHEN0220/SegSwap">[Code]</a> &nbsp;
              <a href="https://youtu.be/9pKwNGZPDr8">[Youtube Video]</a> &nbsp;
              <a href="http://imagine.enpc.fr/~shenx/SegSwap/slides.pdf">[Slides]</a> &nbsp;
              <a class="github-button" href="https://github.com/XiSHEN0220/SegSwap" data-icon="octicon-star" data-size="medium" data-show-count="true">Star</a>  
              

            </td>
        </tr>

        <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div style="display: flex; justify-content: center;">
                <img src='images/teaser_SSR.jpg' width="160" >
              </div>
              
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="http://imagine.enpc.fr/~shenx/SSR/">
                <papertitle>Re-ranking for image retrieval and transductive few-shot classification</papertitle>
              </a>
              </br>
              
              <strong>Xi Shen</strong>,
              <a href="https://youngxiao13.github.io/">Yang Xiao</a>,
            <a href="http://hushell.github.io/">Shell Xu Hu</a>,
            <a href="https://www.sbaiothman.com/">Othman Sbai</a>,
            <a href="http://imagine.enpc.fr/~aubrym/">Mathieu Aubry</a>
              </br>
              <em>NeurIPS</em>, 2021
              </br>
              <a href="http://imagine.enpc.fr/~shenx/SSR/">[Project page]</a> &nbsp;
              <a href="https://papers.nips.cc/paper/2021/file/d9fc0cdb67638d50f411432d0d41d0ba-Paper.pdf">[PDF]</a> &nbsp;
              <a href="https://github.com/XiSHEN0220/SSR">[Code]</a> &nbsp;
              <a href="http://imagine.enpc.fr/~shenx/SSR/ssr.mp4">[Video]</a> &nbsp;
              <a class="github-button" href="https://github.com/XiSHEN0220/SSR" data-icon="octicon-star" data-size="medium" data-show-count="true">Star</a>  
            </td>
        </tr>

        <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div style="display: flex; justify-content: center;">
                <img src='images/collation.jpeg' width="160" >
              </div>
              
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="http://imagine.enpc.fr/~shenx/ImageCollation/">
                <papertitle>Image Collation: Matching illustrations in manuscripts</papertitle>
              </a>
              </br>
              
            <a href="https://github.com/Rykoua">Ryad Kaoua</a>,
            <strong>Xi Shen</strong>,
            <a href="https://www.orient-mediterranee.com/spip.php?article487&lang=fr">Stavros Lazaris</a>,
            <a href="https://davidpicard.github.io/">David Picard</a>,
            <a href="http://imagine.enpc.fr/~aubrym/">Mathieu Aubry</a>
              </br>
              <em>ICDAR</em>, 2021
              </br>
              <a href="http://imagine.enpc.fr/~shenx/ImageCollation/">[Project page]</a> &nbsp;
              <a href="https://arxiv.org/abs/2108.08109">[PDF]</a> &nbsp;
              <a href="https://github.com/Rykoua/ImageCollation">[Code]</a> &nbsp;
              <a href="https://github.com/Rykoua/ImageCollation/tree/main/ManuscriptDownloader">[Dataset]</a> &nbsp;
              <a href="https://youtu.be/yXe7JHSJDUs">[Video]</a> &nbsp;
              <a href="http://imagine.enpc.fr/~shenx/ImageCollation/ICDAR_ImageCollation_Slides.pdf"> [Slides] </a> &nbsp;
              <a href="http://imagine.enpc.fr/~shenx/ImageCollation/icdar21.pdf"> [Poster] </a> &nbsp;
              <a class="github-button" href="https://github.com/Rykoua/ImageCollation" data-icon="octicon-star" data-size="medium" data-show-count="true">Star</a>  
              
            </td>
        </tr>

        <tr bgcolor="#ffffd0">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div style="display: flex; justify-content: center;">
                <img src='images/medici_small.gif' width="80" > &nbsp;
                <img src='images/RANSACflow_small.gif' width="80" >
                
              </div>
              
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="http://imagine.enpc.fr/~shenx/RANSAC-Flow/">
                <papertitle>RANSAC-Flow: generic two-stage image alignment</papertitle>
              </a>
              </br>
              
            <strong>Xi Shen</strong>,
            <a href="https://imagine-lab.enpc.fr/staff-members/francois-darmon">Fran&ccedil;ois  Darmon</a>,
            <a href="http://people.eecs.berkeley.edu/~efros/">Alexei A. Efros</a>,
            <a href="http://imagine.enpc.fr/~aubrym/">Mathieu Aubry</a>
              </br>
              <em>ECCV</em>, 2020
              </br>
              <a href="http://imagine.enpc.fr/~shenx/RANSAC-Flow/">[Project page]</a> &nbsp;
              <a href="https://arxiv.org/abs/2004.01526">[PDF]</a> &nbsp;
              <a href="https://github.com/XiSHEN0220/RANSAC-Flow">[Code]</a> &nbsp;
              <a href="https://youtu.be/ltZpqRtuA6A">[Demo]</a> &nbsp;
              <a href="https://youtu.be/FXqCZlmipdM">[Video]</a> &nbsp;
              <a href="http://imagine.enpc.fr/~shenx/RANSAC-Flow/slide_ransac_flow.pptx"> [Slides] </a> &nbsp;
              <a class="github-button" href="https://github.com/XiSHEN0220/RANSAC-Flow" data-icon="octicon-star" data-size="medium" data-show-count="true">Star</a>  
              
            </td>
        </tr>

        <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div style="display: flex; justify-content: center;">
                <img src='images/sib.jpeg' width="160" >
                
              </div>
              
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://github.com/hushell/sib_meta_learn">
                <papertitle>Empirical Bayes Transductive Meta-Learning with Synthetic Gradients</papertitle>
              </a>
              </br>
            
            <a href="http://hushell.github.io/">Shell Xu Hu</a>,
            <a href="https://www.linkedin.com/in/pablo-garc%C3%ADa-moreno-4b812087/?originalSubdomain=uk">Pablo Moreno</a>,
            <a href="https://youngxiao13.github.io/">Yang Xiao</a>,
            <strong>Xi Shen</strong>,
            <a href="http://imagine.enpc.fr/~obozinsg/">Guillaume Obozinski</a>,
            <a href="https://inverseprobability.com/biog">Neil D. Lawrence</a> 
            <a href="http://adamian.github.io/">Andreas Damianou</a>
              </br>
              <em>ICLR</em>, 2020
              </br>
              <a href="https://openreview.net/forum?id=Hkg-xgrYvH">[PDF]</a> &nbsp;
              <a href="https://github.com/hushell/sib_meta_learn">[Code]</a> &nbsp;
              <a class="github-button" href="https://github.com/hushell/sib_meta_learn" data-icon="octicon-star" data-size="medium" data-show-count="true">Star</a>  
              
            </td>
        </tr>

        <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div style="display: flex; justify-content: center;">
                <img src='images/watermark.png' width="160" > 
                
              </div>
              
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="http://imagine.enpc.fr/~shenx/Watermark/">
                <papertitle>Large-Scale Historical Watermark Recognition: dataset and a new consistency-based approach</papertitle>
              </a>
              </br>
              
            <strong>Xi Shen</strong>,
            <a href="http://www.chartes.psl.eu/en/ilaria-pastrolin">Ilaria Pastrolin</a>,
            <a href="https://scholar.google.com/citations?user=B2RS1M4AAAAJ&hl=en">Oumayma Bounou</a>,
            <a href="https://scholar.google.fr/citations?user=7atfg7EAAAAJ&hl=en">Spyros Gidaris</a>,
            <a href="http://www.chartes.psl.eu/fr/marc-smith">Marc Smith</a>,
            <a href="http://www.chartes.psl.eu/fr/olivier-poncet">Olivier Poncet</a>,
            <a href="http://imagine.enpc.fr/~aubrym/">Mathieu Aubry</a>
              </br>
              <em>ICPR</em>, 2020
              </br>
              <a href="http://imagine.enpc.fr/~shenx/Watermark/">[Project page]</a> &nbsp;
              <a href="http://arxiv.org/pdf/1908.10254.pdf">[PDF]</a> &nbsp;
              <a href="https://github.com/XiSHEN0220/WatermarkReco">[Code]</a> &nbsp;
              <a href="http://imagine.enpc.fr/~shenx/data/Watermark.zip">[Dataset]</a> &nbsp;
              <a href="https://www.youtube.com/embed/9Y47oyvjfQ8">[Video]</a> &nbsp;
              <a href="http://imagine.enpc.fr/~shenx/Watermark/watermarkReco.pptx">[Slides]</a> &nbsp;
              <a href="https://filigranes.inria.fr/#/filigrane-search">[Web App]</a>&nbsp;
              <a href="https://hal.archives-ouvertes.fr/hal-02513038/">[Paper on Web App]</a> &nbsp;
              <a class="github-button" href="https://github.com/XiSHEN0220/WatermarkReco" data-icon="octicon-star" data-size="medium" data-show-count="true">Star</a> 
              
              
            </td>
        </tr>


        <tr bgcolor="#ffffd0">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div style="display: flex; justify-content: center;">
                <img src='images/artminer.jpeg' width="160" >
                
              </div>
              
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="http://imagine.enpc.fr/~shenx/ArtMiner/">
                <papertitle>Discovering Visual Patterns in Art Collections with Spatially-consistent Feature Learning</papertitle>
              </a>
              </br>
              
            <strong>Xi Shen</strong>,
            <a href="http://people.eecs.berkeley.edu/~efros/">Alexei A. Efros</a>,
            <a href="http://imagine.enpc.fr/~aubrym/">Mathieu Aubry</a>
              </br>
              <em>CVPR</em>, 2019
              </br>
              <a href="https://www.nature.com/articles/d41586-019-01794-3">
              Covered by Nature </a>
              </br>
              <a href="http://imagine.enpc.fr/~shenx/ArtMiner/">[Project page]</a> &nbsp;
              <a href="https://arxiv.org/abs/1903.02678">[PDF]</a> &nbsp;
              <a href="https://github.com/XiSHEN0220/ArtMiner">[Code]</a> &nbsp;
              <a href="http://imagine.enpc.fr/~shenx/data/Brueghel.zip">[Dataset]</a> &nbsp;
              <a href="http://imagine.enpc.fr/~shenx/ArtMiner/artMiner.pdf"> [Slides] </a> &nbsp;
              <a class="github-button" href="https://github.com/XiSHEN0220/ArtMiner" data-icon="octicon-star" data-size="medium" data-show-count="true">Star</a> 
              
            </td>
        </tr>


        <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div style="display: flex; justify-content: center;">
                <img src='images/maan.png' width="160" >
                
              </div>
              
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://github.com/yyuanad/MAAN">
                <papertitle>Marginalized Average Attentional Network For Weakly-Supervised Learning</papertitle>
              </a>
              </br>
            
            <a href="https://yyuanad.github.io/">Yuan Yuan</a>, 
            <a href="https://yueminglyu.github.io/">Yueming Lyu</a>,   
            <strong>Xi Shen</strong>,
            <a href="https://www.uts.edu.au/staff/ivor.tsang">Ivor Tsang</a>,
            <a href="https://sites.google.com/view/dyyeung/">Dit-Yan Yeung</a>
              </br>
              <em>ICLR</em>, 2019
              </br>
              <a href="https://openreview.net/pdf?id=HkljioCcFQ">[PDF]</a> &nbsp;
              <a href="https://github.com/yyuanad/MAAN">[Code]</a> &nbsp;
              <a class="github-button" href="https://github.com/yyuanad/MAAN" data-icon="octicon-star" data-size="medium" data-show-count="true">Star</a> 
              
            </td>
        </tr>
    </tbody></table>



          
    

				
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Misc.</heading>
              <hr/>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
          <h3> Academic Services </h3>
          <!--
          <ul>
          <li> ICML: 2022</li>
          <li> CVPR: 2022</li>
          <li> NeurIPS: 2021-2022</li>
          <li> ICLR: 2020 - 2021</li>
          <li> BMVC: 2019 - 2022</li>
          </ul>
          -->
					<p>&nbsp; &nbsp; &nbsp; &nbsp; ICML 2022 (Outstanding Reviewer), CVPR 2022 - 2023, ECCV 2022, NeurIPS 2021 - 2022, ICLR 2020 - 2023, BMVC 2019 - 2022, WACV 2023. </p>
          <h3> Teacher Assistant </h3>
          <ul>
          <li> 
            <b style="background-color:#ddd;">03 / 2021 - 04 / 2021 &emsp; <a href="https://psl.eu/en"> Université PSL</a> (Paris Sciences & Lettres)</b>&emsp;
            <a href="https://data-psl.github.io/intensive-week-dhai-2022/">Digital Humanities Meet Artificial Intelligence</a> (M1).
          </li>
          <li> 
            <b style="background-color:#ddd;">02 / 2021 - 06 / 2021 &emsp; <a href="https://www.ecoledesponts.fr">Ecole des Ponts ParisTech</a>  </b>&emsp;
            Computer Vision for Mechanics of materials (L3).
          </li>
          <li> <b style="background-color:#ddd;">02 / 2019 - 06 / 2019 &emsp; <a href="https://www.ecoledesponts.fr">Ecole des Ponts ParisTech</a>  </b>&emsp; Traitement de l'information et vision artificielle (M1).
          </li>
          
          </ul>

          <h3> Sports</h3>
          <p>&nbsp; &nbsp; &nbsp; &nbsp; I am an amateur football player, a video of highlights can be found here.  I also like jogging, hiking, camping and ski. </p>

          <h3> Coding</h3>
          <p>&nbsp; &nbsp; &nbsp; &nbsp; All of my released code is maintained on my <a href="https://github.com/XiSHEN0220">GitHub account </a>.  </p>
          <table><tbody>
              
              
              <td width="10%"><p> <a class="github-button" href="https://github.com/XiSHEN0220/RANSAC-Flow" data-icon="octicon-star" data-size="large" data-show-count="true">Star</a></p></td>
              <td width="10%"><p> <a class="github-button" href="https://github.com/XiSHEN0220/RANSAC-Flow/fork" data-icon="octicon-repo-forked" data-size="large" data-show-count="true">Fork</a></p></td>
              <td width="20%">RANSAC-Flow</td>
             

             
             <td width="10%"><p> <a class="github-button" href="https://github.com/XiSHEN0220/ArtMiner" data-icon="octicon-star" data-size="large" data-show-count="true">Star</a></p></td>
              <td width="10%"><p> <a class="github-button" href="https://github.com/XiSHEN0220/ArtMiner/fork" data-icon="octicon-repo-forked" data-size="large" data-show-count="true">Fork</a></p></td>
              <td width="20%">ArtMiner</td>
             
        </tbody></table>
        <table><tbody>
             <td width="10%"><p> <a class="github-button" href="https://github.com/YangtaoWANG95/TokenCut" data-icon="octicon-star" data-size="large" data-show-count="true">Star</a></p></td>
              <td width="10%"><p> <a class="github-button" href="https://github.com/YangtaoWANG95/TokenCut/fork" data-icon="octicon-repo-forked" data-size="large" data-show-count="true">Fork</a></p></td>
              <td width="20%">TokenCut</td>


             

              <td width="10%"><p> <a class="github-button" href="https://github.com/hushell/sib_meta_learn" data-icon="octicon-star" data-size="large" data-show-count="true">Star</a></p></td>
              <td width="10%"><p> <a class="github-button" href="https://github.com/hushell/sib_meta_learn/fork" data-icon="octicon-repo-forked" data-size="large" data-show-count="true">Fork</a></p></td>
              <td width="20%">SIB</td>
        </tbody></table>
        <table><tbody>
             <td width="10%"><p> <a class="github-button" href="https://github.com/XiSHEN0220/SegSwap" data-icon="octicon-star" data-size="large" data-show-count="true">Star</a></p></td>
              <td width="10%"><p> <a class="github-button" href="https://github.com/XiSHEN0220/SegSwap/fork" data-icon="octicon-repo-forked" data-size="large" data-show-count="true">Fork</a></p></td>
              <td width="20%">SegSwap</td>

              <td width="10%"><p> <a class="github-button" href="https://github.com/XiSHEN0220/SSR" data-icon="octicon-star" data-size="large" data-show-count="true">Star</a></p></td>
              <td width="10%"><p> <a class="github-button" href="https://github.com/XiSHEN0220/SSR/fork" data-icon="octicon-repo-forked" data-size="large" data-show-count="true">Fork</a></p></td>
              <td width="20%">SSR</td>
        </tbody></table>

        <table><tbody>
             <td width="10%"><p> <a class="github-button" href="https://github.com/yingyichen-cyy/Nested-Co-teaching" data-icon="octicon-star" data-size="large" data-show-count="true">Star</a></p></td>
              <td width="10%"><p> <a class="github-button" href="https://github.com/yingyichen-cyy/Nested-Co-teaching/fork" data-icon="octicon-repo-forked" data-size="large" data-show-count="true">Fork</a></p></td>
              <td width="20%">NCT</td>

              <td width="10%"><p> <a class="github-button" href="https://github.com/XiSHEN0220/WatermarkReco" data-icon="octicon-star" data-size="large" data-show-count="true">Star</a></p></td>
              <td width="10%"><p> <a class="github-button" href="https://github.com/XiSHEN0220/WatermarkReco/fork" data-icon="octicon-repo-forked" data-size="large" data-show-count="true">Fork</a></p></td>
              <td width="20%">WatermarkReco</td>

              
        </tbody></table>

        <table><tbody>
             <td width="10%"><p> <a class="github-button" href="https://github.com/dulucas/siMLPe" data-icon="octicon-star" data-size="large" data-show-count="true">Star</a></p></td>
              <td width="10%"><p> <a class="github-button" href="https://github.com/dulucas/siMLPe/fork" data-icon="octicon-repo-forked" data-size="large" data-show-count="true">Fork</a></p></td>
              <td width="20%">siMLPe</td>

              <td width="10%"><p> <a class="github-button" href="https://github.com/Mael-zys/T2M-GPT" data-icon="octicon-star" data-size="large" data-show-count="true">Star</a></p></td>
              <td width="10%"><p> <a class="github-button" href="https://github.com/Mael-zys/T2M-GPT/fork" data-icon="octicon-repo-forked" data-size="large" data-show-count="true">Fork</a></p></td>
              <td width="20%">T2M-GPT</td>
              


              
        </tbody></table>
        

        </br>
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <hr/>
      <tr>
        <td>
        <p>
        <a href="https://www.easycounter.com/"><img src="https://www.easycounter.com/counter.php?shenxi"
            border="0" alt="Website Hit Counter"></a>
        <font size="2">unique visitors since Sep 2023</p>
        </td>

        <td>
        <p align="right"><font size="2">
          This website takes the template from <a href="https://github.com/jonbarron/website">here</a>.
        </font>

        </p>
        </td>
      </tr>
      </table>


        

          

          

          </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
