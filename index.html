<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Xi Shen</title>
  
  <meta name="author" content="Xi Shen">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <style>
    #more {display: none;}
  </style>
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/web_icon.jpeg">
<script async defer src="https://buttons.github.io/buttons.js"></script>
<script src="https://ajax.googleapis.com/ajax/libs/jquery/2.1.1/jquery.min.js"></script>
</head>

<script>
function LoadMore() {
  var dots = document.getElementById("dots");
  var moreText = document.getElementById("more");
  var btnText = document.getElementById("myBtn");

  if (dots.style.display === "none") {
    dots.style.display = "inline";
    btnText.innerHTML = "SHOW MORE..."; 
    moreText.style.display = "none";
  } else {
    dots.style.display = "none";
    btnText.innerHTML = "HIDE..."; 
    moreText.style.display = "inline";
  }
}
</script>



<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Xi Shen</name> 
              </p>

              <p style="text-align:center">
                Email: <i><u>shenxiluc at gmail dot com</u></i> 
              </p>
              
              <p>
              <b>Bio:</b>
              I am the Co-founder and Chief Scientist at <a href="https://www.intellindust.com/">Intellindust</a>, where our team (<a href="https://intellindust-ai-lab.github.io/">Intellindust AI Lab</a>) develops reliable and efficient computer vision algorithms for real-time applications on edge devices with limited NPU computation.
            </p>

            <p>
              I completed my Ph.D. in the <a href="http://imagine.enpc.fr/">IMAGINE team</a> at <a href="http://www.enpc.fr/">√âcole des Ponts ParisTech</a>. Before that, I was a Senior Researcher at Tencent AI Lab in Shenzhen, China.
            </p>

            <p>
              If you‚Äôre interested in an internship focusing on safety AI or efficient training for real-time vision applications, feel free to reach out by email. 

            </p>

            <p>
              (Last update 09/11/2025)
            </p>


              <p style="text-align:center">
              <div id="circles_container">
                  <a href='http://github.com/XiSHEN0220'><img class="iconImg" src="images/github.png" alt="Github">GitHub</a>
                  <a href='Xi_Shen_CV.pdf'><img class="iconImg" src="images/cv.png" alt="CV">CV</a>
                  <a href='https://scholar.google.com/citations?user=nKSXus4AAAAJ&hl=en'><img class="iconImg" src="images/googleScholar.png" alt="Google Scholar">Google Scholar</a>
                  
                </div>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/xishen.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/xishen.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>


  <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
  <tr><td style="padding:20px;width:100%;vertical-align:middle">
              <heading>üì∞ News</heading>
  <hr/>
    
        <ul style="list-style:none; padding-left:0; line-height:1.6; font-size:1em;">
        <li>
          <b style="background-image: linear-gradient(to top, #f3e7e9 0%, #e3eeff 99%, #e3eeff 100%);">04 / 02 / 2026</b>&emsp;
           <a href="https://intellindust-ai-lab.github.io/projects/FSOD-VFM/">FSOD-VFM</a> is accepted by ICLR 2026.
        </li>
    
        <li>
          <b style="background-image: linear-gradient(to top, #f3e7e9 0%, #e3eeff 99%, #e3eeff 100%);">26 / 11 / 2025</b>&emsp;
           <a href="https://pokerman8.github.io/SKEL-CF/">SKEL-CF</a> is now available on arXiv.
        </li>
  
        <li>
          <b style="background-image: linear-gradient(to top, #f3e7e9 0%, #e3eeff 99%, #e3eeff 100%);">20 / 11 / 2025</b>&emsp;
           <a href="https://ocean146.github.io/SimROD2025/">SimROD: A Simple Baseline for Raw Object Detection with Global and Local Enhancements</a> is accepted by AAAI 2026.
        </li>

        <li>
          <b style="background-image: linear-gradient(to top, #f3e7e9 0%, #e3eeff 99%, #e3eeff 100%);">20 / 09 / 2025</b>&emsp;
           <a href="https://nifangbaage.github.io/Explicit-Visual-Prompt/">Explicit visual prompting for universal foreground segmentations</a> is accepted by TPAMI.
        </li>
        

        <li>
          <b style="background-image: linear-gradient(to top, #f3e7e9 0%, #e3eeff 99%, #e3eeff 100%);">20 / 09 / 2025</b>&emsp;
           <a href="https://intellindust-ai-lab.github.io/projects/DEIMv2/">"[DEIMv2] Real-Time Object Detection Meets DINOv3"</a> is now available on arXiv.
        </li>
          
        <li>
          <b style="background-image: linear-gradient(to top, #f3e7e9 0%, #e3eeff 99%, #e3eeff 100%);">01 / 07 / 2025</b>&emsp;
           We took <a href="https://intellindust-ai-lab.github.io/#:~:text=Track%201%20Joint-,Denoising,-and%20Demosaicing%20challenge">2nd place in the NTIRE Joint Denoising and Demosaicing Challenge</a>, <a href="https://intellindust-ai-lab.github.io/#:~:text=Ball%20Action%20Spotting-,challenge,-CVsports%20Workshop%2C%20CVPR">2nd place in the SoccerNet Ball Action Spotting Challenge</a>, and received <a href="https://intellindust-ai-lab.github.io/#:~:text=Honorable%20Mention%20in-,Foundational,-Few%2DShot%20Object">an honorable mention in the CVPR 2025 Foundation Few-shot Object Detection Challenge</a>.
        </li>
        
        <li>
          <b style="background-image: linear-gradient(to top, #f3e7e9 0%, #e3eeff 99%, #e3eeff 100%);">27 / 02 / 2025</b>&emsp;
           <a href="https://www.shihuahuang.cn/DEIM/">"DEIM: DETR with Improved Matching for Fast Convergence"</a> is accepted by CVPR 2025.
        </li>
        
        <li>
          <b style="background-image: linear-gradient(to top, #f3e7e9 0%, #e3eeff 99%, #e3eeff 100%);">03 / 10 / 2024</b>&emsp;
           <a href="https://www.nature.com/articles/s41565-024-01753-8">"Designing nanotheranostics with machine learning"</a> is accepted by Nature Nanotechnology 2024.
        </li>
          
        <li>
          <b style="background-image: linear-gradient(to top, #f3e7e9 0%, #e3eeff 99%, #e3eeff 100%);">01 / 10 / 2024</b>&emsp;
           Building on our CVPR work <a href="https://yutingli0606.github.io/SURE/">SURE</a>, we won 1st place in the Open-set Recognition challenge at the <a href="https://www.ood-cv.org/challenge.html">Out Of Distribution Generalization in Computer Vision Workshop, ECCV 2024</a>.
        </li>
        <li>
          <b style="background-image: linear-gradient(to top, #f3e7e9 0%, #e3eeff 99%, #e3eeff 100%);">01 / 09 / 2024</b>&emsp;
          We release <a href="https://arxiv.org/abs/2409.07907">COCO-FP</a>, a dataset designed to evaluate false positive detections for COCO object detectors.
        </li>
        
        <li>
          <b style="background-image: linear-gradient(to top, #f3e7e9 0%, #e3eeff 99%, #e3eeff 100%);">01 / 09 / 2024</b>&emsp;
          <a href="https://yutingli0606.github.io/HTR-VT/">"HTR-VT"</a> is accepted by Pattern Recognition 2024. 
        </li>
        <li>
          <b style="background-image: linear-gradient(to top, #f3e7e9 0%, #e3eeff 99%, #e3eeff 100%);">01 / 06 / 2024</b>&emsp;
           We achieved 1st place in Low-light Object Detection and 3rd place in Instance Segmentation at <a href="https://pbdl-ws.github.io/pbdl2024/">the 4th Physics-Based Vision meets Deep Learning Workshop, CVPR 2024</a>.
        </li>
          
        <li>
          <b style="background-image: linear-gradient(to top, #f3e7e9 0%, #e3eeff 99%, #e3eeff 100%);">27 / 02 / 2024</b>&emsp;
           2/2 papers are accepted by CVPR 2024, thanks to all my co-authors!
        </li>
        <span id="dots"> </span>
        <span id="more">
        
        <li>
          <b style="background-image: linear-gradient(to top, #f3e7e9 0%, #e3eeff 99%, #e3eeff 100%);">01 / 08 / 2023</b>&emsp;
           <a href="https://www.m-psi.fr/Papers/TokenCut2022/">"TokenCut"</a> is accepted by TPAMI.
        </li>
        <li>
          <b style="background-image: linear-gradient(to top, #f3e7e9 0%, #e3eeff 99%, #e3eeff 100%);">01 / 07 / 2023</b>&emsp;
           <a href="https://arxiv.org/abs/2309.09294">"LivelySpeaker"</a> is accepted by ICCV 2023.
        </li>
        <li>
          <b style="background-image: linear-gradient(to top, #f3e7e9 0%, #e3eeff 99%, #e3eeff 100%);">01 / 03 / 2023</b>&emsp;
           3/3 papers are accepted by CVPR 2023, thanks to all my co-authors!
        </li>  
        <li>
          <b style="background-image: linear-gradient(to top, #f3e7e9 0%, #e3eeff 99%, #e3eeff 100%);">15 / 01 / 2023</b>&emsp;
           Our paper <a href="https://mael-zys.github.io/T2M-GPT/">T2M-GPT: Generating Human Motion from Textual Descriptions with Discrete Representations</a> is available on arXiv.
        </li>
        <li>
          <b style="background-image: linear-gradient(to top, #f3e7e9 0%, #e3eeff 99%, #e3eeff 100%);">22 / 11 / 2022</b>&emsp;
           Our paper <a href="https://sadtalker.github.io/">SadTalker: Learning Realistic 3D Motion Coefficients for Stylized Audio-Driven Single Image Talking Face Animation</a> is available on arXiv.
        </li>
        
        <li>
          <b style="background-image: linear-gradient(to top, #f3e7e9 0%, #e3eeff 99%, #e3eeff 100%);">30 / 08 / 2022</b>&emsp;
           Our paper <a href="https://arxiv.org/abs/2207.01567">Back to MLP: A Simple Baseline for Human Motion Prediction (siMLPe)</a> is accepted to WACV 2023.
        </li>

        <li>
          <b style="background-image: linear-gradient(to top, #f3e7e9 0%, #e3eeff 99%, #e3eeff 100%);">14 / 07 / 2022</b>&emsp;
           I am honoured to be acknowledged as Outstanding Reviewer for ICML 2022.
        </li>
        
        <li>
          <b style="background-image: linear-gradient(to top, #f3e7e9 0%, #e3eeff 99%, #e3eeff 100%);">27 / 06 / 2022</b>&emsp;
           Our paper <a href="https://yingyichen-cyy.github.io/CompressFeatNoisyLabels/">Compressing Features for Learning with Noisy Labels</a> is accepted to TNNLS 2022.
        </li>
        <li>
          <b style="background-image: linear-gradient(to top, #f3e7e9 0%, #e3eeff 99%, #e3eeff 100%);">19 / 06 / 2022</b>&emsp;
           CVPR 2022 workshop presentations: <a href="https://www.m-psi.fr/Papers/TokenCut2022/">"TokenCut"</a> in <a href="https://sites.google.com/view/l3d-ivu/">"L3D"</a>;  <a href="http://imagine.enpc.fr/~shenx/SegSwap/">"SegSwap"</a> in <a href="https://image-matching-workshop.github.io/">"Image Matching"</a> and <a href="https://sites.google.com/view/t4v-cvpr22">"Transformer"</a>.
        </li>  
          
        <li>
          <b style="background-image: linear-gradient(to top, #f3e7e9 0%, #e3eeff 99%, #e3eeff 100%);">02 / 03 / 2022</b>&emsp;
           <a href="https://www.m-psi.fr/Papers/TokenCut2022/">"TokenCut"</a> is accepted to CVPR 2022. Check <a href="https://huggingface.co/spaces/akhaliq/TokenCut">Gradio Demo</a> here. 
        </li>  
        <li>
          <b style="background-image: linear-gradient(to top, #f3e7e9 0%, #e3eeff 99%, #e3eeff 100%);">25 / 02 / 2022</b>&emsp;
          Our paper <a href="https://www.m-psi.fr/Papers/TokenCut2022/">"Self-Supervised Transformers for Unsupervised Object Discovery using Normalized Cut"</a> (TokenCut) is published on arXiv.
        </li>
        <li>
          <b style="background-image: linear-gradient(to top, #f3e7e9 0%, #e3eeff 99%, #e3eeff 100%);">13 / 01 / 2022</b>&emsp;
          Our paper <a href="http://imagine.enpc.fr/~shenx/HisImgAnalysis/">"Spatially-consistent Feature Matching and Learning for Heritage Image Analysis"</a> is accepted to <a href="https://nips.cc/">IJCV</a>.
        </li>
        <li>
          <b style="background-image: linear-gradient(to top, #f3e7e9 0%, #e3eeff 99%, #e3eeff 100%);">28 / 12 / 2021</b>&emsp;
          I join <a href="https://ai.tencent.com">Tencent AI Lab</a> as a Senior Researcher, I will work with <a href="https://juew.org/">Dr. Jue Wang</a> on Computer Vision.
        </li>
        <li>
          <b style="background-image: linear-gradient(to top, #f3e7e9 0%, #e3eeff 99%, #e3eeff 100%);">03 / 12 / 2021</b>&emsp;
          I am defending my Ph.D. today! <a href="Xi_Shen_defense_slides.pdf">[Slides]</a> <a href="Xi_Shen_thesis.pdf">[Thesis]</a>
        </li>
        <li>
          <b style="background-color:#ddd;">29 / 10 / 2021</b>&emsp;
          Our paper <a href="http://imagine.enpc.fr/~shenx/SegSwap/">"Learning Co-segmentation by Segment Swapping for Retrieval and Discovery"</a> is published on arXiv.
        </li> 
        <li>
          <b style="background-image: linear-gradient(to top, #f3e7e9 0%, #e3eeff 99%, #e3eeff 100%);">29 / 09 / 2021</b>&emsp;
          Our paper <a href="http://imagine.enpc.fr/~shenx/SSR/">"Re-ranking for image retrieval and transductive few-shot classification"</a> is accepted to <a href="https://nips.cc/">Neurips 2021</a>.
        </li>
        
          <li>
          <b style="background-image: linear-gradient(to top, #f3e7e9 0%, #e3eeff 99%, #e3eeff 100%);">18 / 04 / 2021</b>&emsp;
          Our paper "<a href="https://imagine.enpc.fr/~shenx/ImageCollation/"> Image Collation: Matching illustrations in manuscripts</a>" is accepted to <a href="https://icdar2021.org/">ICDAR 2021</a>.
        </li>
          
          <li>
          <b style="background-image: linear-gradient(to top, #f3e7e9 0%, #e3eeff 99%, #e3eeff 100%);">15 / 07 / 2020</b>&emsp;
          Our paper "<a href="http://imagine.enpc.fr/~shenx/RANSAC-Flow/"> RANSAC-Flow: generic two-stage image alignment</a>" is accepted to <a href="https://eccv2020.eu/">ECCV 2020</a>. Watch the <a href="http://imagine.enpc.fr/~shenx/RANSAC-Flow/img/demo_ransac_flow.mp4"> demo</a>.
        </li>
        <li>
          <b style="background-color:#ddd;">07 / 03 / 2020</b>&emsp;
          The web application of historical watermark recognition is online!! Check the <a href="https://hal.inria.fr/hal-02513038/document"> paper </a> and the  <a href="https://filigranes.inria.fr/#/filigrane-search"> web application </a>.
        </li> 
        <li>
          <b style="background-color:#ddd;">12 / 12 / 2019</b>&emsp;
          Our paper "<a href="https://openreview.net/forum?id=Hkg-xgrYvH"> Empirical Bayes Transductive Meta-Learning with Synthetic Gradients</a>" accepted to <a href="https://iclr.cc/Conferences/2020">ICLR 2020</a>.
        </li> 
        <li>
          <b style="background-color:#ddd;">06 / 08 / 2019</b>&emsp;
          Our paper "<a href="http://imagine.enpc.fr/~shenx/Watermark"> Large-Scale Historical Watermark Recognition: dataset and a new consistency-based approach</a>" is published on arXiv.
        </li> 
        
        </span>
        <div style="text-align:center">
        <button class="button-2" role="button" onclick="LoadMore()" id="myBtn" >SHOW MORE...</button>
      </div>
        </ul>
  </tbody></table>


<table style="width:100%; border:0; border-spacing:0; border-collapse:separate; margin:0 auto;">
  <tbody>
    <tr>
      <td style="padding:20px; width:100%; vertical-align:middle;">
        <heading style="font-size:1.5em; font-weight:600;">üèÜ Awards & Honors</heading>
        <hr style="margin:8px 0 16px 0; border:0; border-top:2px solid #ddd;"/>

        <ul style="list-style:none; padding-left:0; line-height:1.6; font-size:1em;">
          
          <!-- Competition Awards -->
          <li style="margin-bottom:12px;">
            <b>ü•á 1st Place</b> ‚Äî Open-set Recognition Challenge,<br/>
            <a href="https://www.ood-cv.org/challenge.html">Out-of-Distribution Generalization in Computer Vision Workshop, ECCV 2024</a><br/>
            <small>(Based on our CVPR work <a href="https://yutingli0606.github.io/SURE/">SURE</a>.)</small>
          </li>

          <li style="margin-bottom:12px;">
            <b>ü•á 1st Place</b> ‚Äî Low-light Object Detection,<br/>
            <a href="https://pbdl-ws.github.io/pbdl2024/">4th Physics-Based Vision Meets Deep Learning Workshop, CVPR 2024</a>
          </li>

          <li style="margin-bottom:12px;">
            <b>ü•â 3rd Place</b> ‚Äî Low-light Instance Segmentation,<br/>
            <a href="https://pbdl-ws.github.io/pbdl2024/">4th Physics-Based Vision Meets Deep Learning Workshop, CVPR 2024</a>
          </li>

          <li style="margin-bottom:12px;">
            <b>ü•à 2nd Place</b> ‚Äî Joint Denoising and Demosaicing,<br/>
            <a href="https://cvlai.net/ntire/2025/">NTIRE Workshop, CVPR 2025</a>
          </li>

          <li style="margin-bottom:12px;">
            <b>ü•à 2nd Place</b> ‚Äî SoccerNet Ball Action Spotting,<br/>
            <a href="https://www.soccer-net.org/challenges/2025">SoccerNet Challenges, CVPR 2025</a>
          </li>

          <li style="margin-bottom:16px;">
            <b>üéñÔ∏è Honorable Mention</b> ‚Äî Foundation Few-shot Object Detection,<br/>
            <a href="https://www.soccer-net.org/challenges/2025">Visual Perception via Learning in an Open World, CVPR 2025</a>
          </li>

          <!-- Personal Recognition -->
          <li style="margin-top:16px;">
            <b>üåü Outstanding Reviewer</b>, 
            <a href="https://icml.cc/">ICML 2022</a>
          </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>üí° Research</heading>
              <hr/>
              <p>
                
                <sup>*</sup> indicates equal contribution. </br>

                <sup>+</sup> indicates corresponding author. </br>

                Representative papers are <span class="highlight">highlighted</span>.
              </p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            
            <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div style="display: flex; justify-content: center;">
                <img src='images/fsod_vfm.png' width="160" >
              </div>
              
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://intellindust-ai-lab.github.io/projects/FSOD-VFM/">
                <papertitle>FSOD-VFM: Few-Shot Object Detection with Vision Foundation Models and Graph Diffusion</papertitle>
              </a>
              </br>
              
              <a href="https://fcbfcb1998.github.io/">Chen-Bin Feng<sup>*</sup></a>,
              Youyang Sha<sup>*</sup>,
              <a href="https://capsule2077.github.io/">Longfei Liu</a>,
              Yongjun Yu,
              <a href="https://www.fst.um.edu.mo/personal/cmvong/">Chi Man Vong<sup>+</sup></a>,
              <a href="https://xuanlong-yu.github.io/">Xuanlong Yu<sup>+</sup></a>,
              <strong>Xi Shen<sup>+</sup></strong>,
              </br>
              <em>ICLR</em>, 2026
              </br>
              <a href="https://intellindust-ai-lab.github.io/projects/FSOD-VFM/">[Project page]</a> &nbsp;
              <a href="https://arxiv.org/abs/2602.03137">[PDF]</a> &nbsp;
              <a href="https://github.com/Intellindust-AI-Lab/FSOD-VFM">[Code (GitHub)]</a> &nbsp;
              <a class="github-button" href="https://github.com/Intellindust-AI-Lab/FSOD-VFM" data-icon="octicon-star" data-size="medium" data-show-count="true">Star</a>

            </td>
            </tr>

            <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div style="display: flex; justify-content: center;">
                <img src='images/skel_cf.gif' width="160" >
              </div>
              
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://pokerman8.github.io/SKEL-CF/">
                <papertitle>SKEL-CF: Coarse-to-Fine Biomechanical Skeleton and Surface Mesh Recovery</papertitle>
              </a>
              </br>
              
              Da Li<sup>*</sup>,
              <a href="https://pokerman8.github.io/">Jiping Jin<sup>*</sup></a>,
              <a href="https://xuanlong-yu.github.io/">Xuanlong Yu</a>,
              Wei Liu, 
              <a href="https://vinthony.github.io/academic/">Xiaodong Cun</a>,
              Kai Chen,
              Rui Fan,
              Jiangang Kong,
              <strong>Xi Shen<sup>+</sup></strong>,
              </br>
              <em>arXiv</em>, 2025
              </br>
              <a href="https://pokerman8.github.io/SKEL-CF/">[Project page]</a> &nbsp;
              <a href="https://arxiv.org/abs/2511.20157">[PDF]</a> &nbsp;
              <a href="https://github.com/Intellindust-AI-Lab/SKEL-CF">[Code (GitHub)]</a> &nbsp;
              <a class="github-button" href="https://github.com/Intellindust-AI-Lab/SKEL-CF" data-icon="octicon-star" data-size="medium" data-show-count="true">Star</a>

            </td>
            </tr>

            <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div style="display: flex; justify-content: center;">
                <img src='images/simrod.png' width="160" >
              </div>
              
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://ocean146.github.io/SimROD2025/">
                <papertitle>SimROD: A Simple Baseline for Raw Object Detection with Global and Local Enhancements</papertitle>
              </a>
              </br>
              
              <a href="https://ocean146.github.io/">Haiyang Xie</a>,
              <strong>Xi Shen</strong>,
              <a href="http://www.shihuahuang.cn/">Shihua Huang</a>,
              <a href="https://qiruiwang0728.github.io/homepage/">Qirui Wang</a>,
              
              <a href="https://wangzwhu.github.io/home/">Zheng Wang</a>
              
              </br>
              <em>AAAI</em>, 2026
              </br>
              <a href="https://ocean146.github.io/SimROD2025/">[Project page]</a> &nbsp;
              <a href="https://arxiv.org/abs/2503.07101">[PDF]</a> &nbsp;
              <a href="https://github.com/ocean146/SimROD/tree/main">[Code (GitHub)]</a>
              <a class="github-button" href="https://github.com/ocean146/SimROD/tree/main" data-icon="octicon-star" data-size="medium" data-show-count="true">Star</a> 
              

            </td>
            </tr>

            <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div style="display: flex; justify-content: center;">
                <img src='images/evp2.png' width="160" >
              </div>
              
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://nifangbaage.github.io/Explicit-Visual-Prompt/">
                <papertitle>Explicit Visual Prompting for Universal Foreground Segmentations</papertitle>
              </a>
              </br>
              
              Weihuang Liu,
              <strong>Xi Shen</strong>,
              <a href="https://www.cis.um.edu.mo/~cmpun/">Chi-Man Pun<sup>+</sup></a>,
              <a href="https://vinthony.github.io/academic/">Xiaodong Cun<sup>+</sup></a>
              
              </br>
              <em>TPAMI</em>, 2025
              </br>
              <a href="https://nifangbaage.github.io/Explicit-Visual-Prompt/">[Project page]</a> &nbsp;
              <a href="https://arxiv.org/pdf/2305.18476">[PDF]</a> &nbsp;
              <a href="https://github.com/NiFangBaAGe/Explicit-Visual-Prompt">[Code (GitHub)]</a>
              <a class="github-button" href="https://github.com/NiFangBaAGe/Explicit-Visual-Prompt" data-icon="octicon-star" data-size="medium" data-show-count="true">Star</a> 
              

            </td>
            </tr>

            <tr bgcolor="#ffffd0">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div style="display: flex; justify-content: center;">
                <img src='images/deimv2.png' width="160" >
              </div>

            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://intellindust-ai-lab.github.io/projects/DEIMv2/">
                <papertitle>Real-Time Object Detection Meets DINOv3</papertitle>
              </a>

              </br>

              <a href="https://www.shihuahuang.cn/">Shihua Huang<sup>*</sup></a>,
              Yongjie Hou<sup>*</sup>,
              <a href="https://capsule2077.github.io/">Longfei Liu<sup>*</sup></a>,
              <a href="https://xuanlong-yu.github.io/">Xuanlong Yu</a>,
              <strong>Xi Shen<sup>+</sup></strong>,
              </br>
              <em>arXiv</em>, 2025
              </br>
              <a href="https://intellindust-ai-lab.github.io/projects/DEIMv2/">[Project page]</a> &nbsp;
              <a href="https://arxiv.org/pdf/2509.20787">[PDF]</a> &nbsp;
              <a href="https://github.com/Intellindust-AI-Lab/DEIMv2">[Code (GitHub)]</a> &nbsp;
              <a class="github-button" href="https://github.com/Intellindust-AI-Lab/DEIMv2" data-icon="octicon-star" data-size="medium" data-show-count="true">Star</a>
            </td>
            </tr>

            <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div style="display: flex; justify-content: center;">
                <img src='images/deim.png' width="160" >
              </div>

            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://www.shihuahuang.cn/DEIM/">
                <papertitle>DEIM: DETR with Improved Matching for Fast Convergence</papertitle>
              </a>

              </br>

              <a href="https://www.shihuahuang.cn/">Shihua Huang</a>,
              <a href="https://scholar.google.com/citations?user=tIFWBcQAAAAJ&hl=en">Zhihao Lu</a>,
              <a href="https://vinthony.github.io/academic/">Xiaodong Cun</a>,
               Yongjun Yu,
               Xiao Zhou,
              <strong>Xi Shen<sup>+</sup></strong>,
              </br>
              <em>CVPR</em>, 2025
              </br>
              <a href="https://www.shihuahuang.cn/DEIM/">[Project page]</a> &nbsp;
              <a href="https://arxiv.org/abs/2412.04234">[PDF]</a> &nbsp;
              <a href="https://www.shihuahuang.cn/DEIM/resrc/DEIM-Slides.pdf">[Slides]</a> &nbsp;
              <a href="https://github.com/ShihuaHuang95/DEIM">[Code (GitHub)]</a> &nbsp;
              <a class="github-button" href="https://github.com/ShihuaHuang95/DEIM" data-icon="octicon-star" data-size="medium" data-show-count="true">Star</a>
            </td>
            </tr>

            <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div style="display: flex; justify-content: center;">
                <img src='images/ml_nano.png' width="160" >
              </div>

            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://www.nature.com/articles/s41565-024-01753-8">
                <papertitle>Designing Nanotheranostics with Machine Learning</papertitle>
              </a>

              </br>

              <a href="https://scholar.google.com/citations?user=ml35-BQAAAAJ&hl=en">Lang Rao</a>,
              <a href="https://yyuanad.github.io/">Yuan Yuan</a>,
              <strong>Xi Shen</strong>,
              Guocan, Yu,
              <a href="https://cde.nus.edu.sg/bme/staff/dr-chen-xiaoyuan-shawn/">Xiaoyuan Chen</a>,
              
              </br>
              <em>Nature Nanotechnology</em>, 2024
              </br>
              <a href="https://www.nature.com/articles/s41565-024-01753-8">[PDF]</a> &nbsp;
            </td>
            </tr>


            <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div style="display: flex; justify-content: center;">
                <img src='images/htr_vt.jpeg' width="160" >
              </div>

            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://yutingli0606.github.io/HTR-VT/">
                <papertitle>HTR-VT: Handwritten Text Recognition with Vision Transformer</papertitle>
              </a>

              </br>

              <a href="https://yutingli0606.github.io/">Yuting Li</a>,
              <a href="https://dexiong.me/">Dexiong Chen</a><sup>+</sup>,
              Tinglong Tang,
              <strong>Xi Shen<sup>+</sup></strong>,
              </br>
              <em>Pattern Recognition</em>, 2024
              </br>
              <a href="https://yutingli0606.github.io/HTR-VT/">[Project page]</a> &nbsp;
              <a href="https://arxiv.org/abs/2409.08573">[PDF]</a> &nbsp;
              <a href="https://github.com/YutingLi0606/HTR-VT">[Code (GitHub)]</a> &nbsp;
              <a class="github-button" href="https://github.com/YutingLi0606/HTR-VT" data-icon="octicon-star" data-size="medium" data-show-count="true">Star</a>
            </td>
            </tr>

            <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div style="display: flex; justify-content: center;">
                <img src='images/dattt_teaser.png' width="160" >
              </div>
              
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://nifangbaage.github.io/DATTT/">
                <papertitle>Depth-aware Test-Time Training for Zero-shot Video Object Segmentation</papertitle>
              </a>
              
              </br>
              
              Weihuang Liu,
              <strong>Xi Shen</strong>,
              Haolun Li,
              Xiuli Bi,
              Bo Liu,
              <a href="https://www.cis.um.edu.mo/~cmpun/">Chi-Man Pun<sup>+</sup></a>,
              <a href="https://vinthony.github.io/academic/">Xiaodong Cun<sup>+</sup></a>
              </br>
              <em>CVPR</em>, 2024
              </br>
              <a href="https://nifangbaage.github.io/DATTT/">[Project page]</a> &nbsp;
              <a href="https://arxiv.org/abs/2403.04258">[PDF]</a> &nbsp;
              <a href="hhttps://github.com/NiFangBaAGe/DATTT">[Code (GitHub)]</a> &nbsp;
              <a class="github-button" href="https://github.com/NiFangBaAGe/DATTT" data-icon="octicon-star" data-size="medium" data-show-count="true">Star</a> 
            </td>
            </tr>

            <tr bgcolor="#ffffd0">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div style="display: flex; justify-content: center;">
                <img src='images/sure.jpeg' width="160" >
              </div>
              
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://yutingli0606.github.io/SURE/">
                <papertitle>SURE: SUrvey REcipes for building reliable and robust deep networks</papertitle>
              </a>
              
              </br>
              
              <a href="https://yutingli0606.github.io/">Yuting Li</a>,
              <a href="https://github.com/yingyichen-cyy/">Yingyi Chen</a>,
              <a href="https://xuanlong-yu.github.io/">Xuanlong Yu</a>,
              <a href="https://dexiong.me/">Dexiong Chen</a><sup>+</sup>,
              <strong>Xi Shen<sup>+</sup></strong>,
              </br>
              <em>CVPR</em>, 2024
              </br>
              <a href="https://www.ood-cv.org/challenge.html">
              <span style="color: firebrick;">
              SURE-OOD is the Winner of SSB-OSR challenge, ECCV 2024 
              </span>
              </a>
              </br>
              <a href="https://yutingli0606.github.io/SURE/">[Project page]</a> &nbsp;
              <a href="https://arxiv.org/pdf/2403.00543.pdf">[PDF]</a> &nbsp;
              <a href="https://github.com/YutingLi0606/SURE">[Code (GitHub)]</a> &nbsp;
              <a class="github-button" href="https://github.com/YutingLi0606/SURE" data-icon="octicon-star" data-size="medium" data-show-count="true">Star</a>
            </br>
              <a href="https://github.com/LIYangggggg/SSB-OSR/blob/main/docs/SURE_OOD.pdf">[Tech. Report SURE-OOD]</a> &nbsp;
              <a href="https://github.com/Intellindust-AI-Lab/SSB-OSR">[Code SURE-OOD (GitHub)]</a> &nbsp;
              <a class="github-button" href="https://github.com/Intellindust-AI-Lab/SSB-OSR" data-icon="octicon-star" data-size="medium" data-show-count="true">Star</a>
               
            </td>
            </tr>

            <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div style="display: flex; justify-content: center;">
                <img src='images/2022_tokencut_video.gif' width="160" >
              </div>
              
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://www.m-psi.fr/Papers/TokenCut2022/">
                <papertitle>TokenCut: Segmenting Objects in Images and Videos with Self-supervised Transformer and Normalized Cut</papertitle>
              </a>
              
              </br>
              
              <a href="https://yangtaowang95.github.io/">Yangtao Wang</a>,
              <strong>Xi Shen<sup>+</sup></strong>,
              <a href="https://yyuanad.github.io/">Yuan Yuan</a>,
              <a href="https://dulucas.github.io/">Yuming Du</a>,
              <a href="https://scholar.google.com/citations?user=ym_t6QYAAAAJ&hl=zh-CN">Maomao Li</a>,
              <a href="http://hushell.github.io">Xu Hu</a>, 
              <a href="http://crowley-coutaz.fr/jlc/jlc.html">James Crowley</a>,
              <a href="https://research.vaufreydaz.org/">Dominique Vaufreydaz</a>
              </br>
              <em>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</em>, 2023
              </br>
              <a href="https://www.m-psi.fr/Papers/TokenCut2022/">[Project page]</a> &nbsp;
              <a href="https://arxiv.org/abs/2209.00383">[PDF]</a> &nbsp;
              <a href="https://github.com/YangtaoWANG95/TokenCut_video">[Code (GitHub)]</a> &nbsp;
              <a class="github-button" href="https://github.com/YangtaoWANG95/TokenCut_video" data-icon="octicon-star" data-size="medium" data-show-count="true">Star</a> 
            </td>
            </tr>

            <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div style="display: flex; justify-content: center;">
                <img src='images/livelyspeaker.gif' width="160" >
              </div>
              
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://github.com/zyhbili/LivelySpeaker">
                <papertitle>LivelySpeaker: Towards Semantic-aware Co-speech Gesture Generation</papertitle>
              </a>
              
              </br>
              
              <a>Yihao Zhi<sup>*</sup></a>,
              <a href="https://vinthony.github.io/academic/">Xiaodong Cun<sup>*</sup></a>,
              <a href="https://xuelin-chen.github.io/">Xuelin Chen</a>,
              <b>Xi Shen</b>,
              <a href="https://guo-w.github.io//">Wen Guo</a>,
              <a>Shaoli Huang</a>,
              <a href="https://svip-lab.github.io/team.html">Shenghua Gao</a>
              </br>
              <em>ICCV</em>, 2023
              </br>
              <a href="https://github.com/zyhbili/LivelySpeaker">[Project page]</a> &nbsp;
              <a href="https://arxiv.org/abs/2309.09294">[PDF]</a> &nbsp;
              <a href="https://github.com/zyhbili/LivelySpeaker">[Code (GitHub)]</a> &nbsp;
              <a class="github-button" href="https://github.com/zyhbili/LivelySpeaker" data-icon="octicon-star" data-size="medium" data-show-count="true">Star</a> 
            </td>
            </tr>

            <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div style="display: flex; justify-content: center;">
                <img src='images/vpt.png' width="160" >
              </div>
              
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://nifangbaage.github.io/Explicit-Visual-Prompt/">
                <papertitle>Explicit Visual Prompting for Low-Level Structure Segmentations</papertitle>
              </a>
              </br>
              
              Weihuang Liu,
              <strong>Xi Shen</strong>,
              <a href="https://www.cis.um.edu.mo/~cmpun/">Chi-Man Pun<sup>+</sup></a>,
              <a href="https://vinthony.github.io/academic/">Xiaodong Cun<sup>+</sup></a>
              
              </br>
              <em>CVPR</em>, 2023
              </br>
              <a href="https://nifangbaage.github.io/Explicit-Visual-Prompt/">[Project page]</a> &nbsp;
              <a href="https://arxiv.org/pdf/2303.10883.pdf">[PDF]</a> &nbsp;
              <a href="https://github.com/NiFangBaAGe/Explicit-Visual-Prompt">[Code (GitHub)]</a>
              <a class="github-button" href="https://github.com/NiFangBaAGe/Explicit-Visual-Prompt" data-icon="octicon-star" data-size="medium" data-show-count="true">Star</a> 
              

            </td>
            </tr>
            <tr bgcolor="#ffffd0">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div style="display: flex; justify-content: center;">
                <img src='images/t2m.gif' width="160" >
              </div>
              
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://mael-zys.github.io/T2M-GPT/">
                <papertitle>T2M-GPT: Generating Human Motion from Textual Descriptions with Discrete Representations</papertitle>
              </a>
              </br>
              
              Jianrong Zhang<sup>*</sup>,
              Yangsong Zhang<sup>*</sup>, 
              
              <a href="https://vinthony.github.io/academic/">Xiaodong Cun</a>,
              <a href="https://scholar.google.com/citations?user=o31BPFsAAAAJ&hl=en">Shaoli Huang</a>,
              <a href="https://yzhang2016.github.io/">Yong Zhang</a>,
              Hongwei Zhao,
              Hongtao Lu,
              <strong>Xi Shen<sup>+</sup></strong>
              </br>
              <em>CVPR</em>, 2023
              </br>
              <a href="https://mael-zys.github.io/T2M-GPT/">[Project page]</a> &nbsp;
              <a href="hhttps://arxiv.org/abs/2301.06052">[PDF]</a> &nbsp;
              <a href="https://github.com/Mael-zys/T2M-GPT">[Code (GitHub)]</a> &nbsp;
              <a href="https://colab.research.google.com/drive/1Vy69w2q2d-Hg19F-KibqG0FRdpSj3L4O?usp=sharing">[Online demo (Colab)]</a> &nbsp;
              <a class="github-button" href="https://github.com/Mael-zys/T2M-GPT" data-icon="octicon-star" data-size="medium" data-show-count="true">Star</a> 
              
              

            </td>
            </tr>
            <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div style="display: flex; justify-content: center;">
                <img src='images/sadtalker.gif' width="160" >
              </div>
              
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://sadtalker.github.io/">
                <papertitle>SadTalker: Learning Realistic 3D Motion Coefficients for Stylized Audio-Driven Single Image Talking Face Animation</papertitle>
              </a>
              </br>
              
              Wenxuan Zhang<sup>*</sup>,
              <a href="https://vinthony.github.io/academic/">Xiaodong Cun</a><sup>*</sup>,
              
              <a href="https://xuanwangvc.github.io/">Xuan Wang</a>,
              <a href="https://yzhang2016.github.io/">Yong Zhang</a>,
              <strong>Xi Shen</strong>,
              Guo Yu,
              Ying Shan,
              Fei Wang
              </br>
              <em>CVPR</em>, 2023
              </br>
              <a href="https://medium.com/voxel51/cvpr-2023-survival-guide-504e965e1f8b">
              <span style="color: firebrick;">
              Top 10 you won't miss papers of CVPR 2023 (Voxel51). 
              </span>
              </a>
              </br>
              <a href="https://sadtalker.github.io/">[Project page]</a> &nbsp;
              <a href="https://arxiv.org/pdf/2211.12194.pdf">[PDF]</a> &nbsp;
              <a href="https://github.com/Winfredy/SadTalker">[Code (GitHub)]</a> &nbsp;
              <a class="github-button" href="https://github.com/Winfredy/SadTalker" data-icon="octicon-star" data-size="medium" data-show-count="true">Star</a> 

            </td>
            </tr>
            <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div style="display: flex; justify-content: center;">
                <img src='images/simple.png' width="160" >
              </div>
              
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://github.com/dulucas/siMLPe">
                <papertitle>Back to MLP: A Simple Baseline for Human Motion Prediction (siMLPe)</papertitle>
              </a>
              </br>
              
              <a href="https://guo-w.github.io//">Wen Guo<sup>*</sup></a>,
              <a href="https://dulucas.github.io/Homepage/">Yuming Du<sup>*</sup></a>, 
              <strong>Xi Shen<sup>+</sup></strong>,
              <a href="https://vincentlepetit.github.io/">Vincent Lepetit</a>,
              <a href="http://xavirema.eu/">Xavier Alameda-Pineda</a>,
              <a href="https://www.iri.upc.edu/people/fmoreno/">Francesc Moreno-Noguer</a>
              
              </br>
              <em>WACV</em>, 2023
              </br>
              <a href="https://github.com/dulucas/siMLPe">[Project page]</a> &nbsp;
              <a href="https://arxiv.org/abs/2207.01567">[PDF]</a> &nbsp;
              <a href="https://github.com/dulucas/siMLPe">[siMLPe Code (GitHub)]</a> &nbsp;
              <a class="github-button" href="https://github.com/dulucas/siMLPe" data-icon="octicon-star" data-size="medium" data-show-count="true">Star</a>  
              

            </td>
            </tr>


            <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div style="display: flex; justify-content: center;">
                <img src='images/sortout.jpeg' width="160" >
              </div>
              
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://yingyichen-cyy.github.io/CompressFeatNoisyLabels/">
                <papertitle>Compressing Features for Learning with Noisy Labels</papertitle>
              </a>
              </br>
              
              <a href="https://github.com/yingyichen-cyy/">Yingyi Chen</a>,
              <a href="http://hushell.github.io">Xu Hu</a>, 
              <strong>Xi Shen<sup>+</sup></strong>,
              <a href="https://myweb.cuhk.edu.cn/chunrongai">Chunrong Ai</a>,
              <a href="https://www.esat.kuleuven.be/sista/members/suykens.html">Johan A.K. Suykens</a>
              </br>
              <em>TNNLS</em>, 2022
              </br>
              <a href="https://yingyichen-cyy.github.io/CompressFeatNoisyLabels/">[Project page]</a> &nbsp;
              <a href="https://yingyichen-cyy.github.io/CompressFeatNoisyLabels/">[PDF]</a> &nbsp;
              <a href="https://github.com/yingyichen-cyy/Nested-Co-teaching">[Code (GitHub)]</a> &nbsp;
              <a href="https://www.youtube.com/watch?v=y9zBDioKMM0&t=5s">[Workshop Video (7 mins)]</a> &nbsp;
              <a class="github-button" href="https://github.com/yingyichen-cyy/Nested-Co-teaching" data-icon="octicon-star" data-size="medium" data-show-count="true">Star</a>  
              

            </td>
            </tr>
            
            <tr bgcolor="#ffffd0">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div style="display: flex; justify-content: center;">
                <img src='images/tokencut.gif' width="160" >
              </div>
              
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://www.m-psi.fr/Papers/TokenCut2022/">
                <papertitle>Self-Supervised Transformers for Unsupervised Object Discovery using Normalized Cut</papertitle>
              </a>
              </br>
              
              <a href="https://yangtaowang95.github.io/">Yangtao Wang</a>,
              <strong>Xi Shen<sup>+</sup></strong>,
              <a href="http://hushell.github.io">Xu Hu</a>, 
                <a href="https://yyuanad.github.io">Yuan Yuan</a>,
                <a href="http://crowley-coutaz.fr/jlc/jlc.html">James Crowley</a>,
                <a href="https://research.vaufreydaz.org/">Dominique Vaufreydaz</a>
              </br>
              <em>CVPR</em>, 2022
              </br>
              <a href="https://www.m-psi.fr/Papers/TokenCut2022/">[Project page]</a> &nbsp;
              <a href="https://arxiv.org/pdf/2202.11539.pdf">[PDF]</a> &nbsp;
              <a href="https://github.com/YangtaoWANG95/TokenCut">[Code (GitHub)]</a> &nbsp;
              <a href="https://colab.research.google.com/github/YangtaoWANG95/TokenCut/blob/master/inference_demo.ipynb">[Colab]</a> &nbsp;
              <a href="https://huggingface.co/spaces/akhaliq/TokenCut">[Gradio Demo]</a> &nbsp;
              <a href="https://gricad-gitlab.univ-grenoble-alpes.fr/wangyan/tokencut">[Code (GitLab)]</a> &nbsp;
              <a class="github-button" href="https://github.com/YangtaoWANG95/TokenCut" data-icon="octicon-star" data-size="medium" data-show-count="true">Star</a>  
              

            </td>
            </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:center">
              <div style="display: flex; justify-content: center;">
                <img src='images/ijcv2022.jpg' width="160">
              </div>
              
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="http://imagine.enpc.fr/~shenx/HisImgAnalysis/">
                <papertitle>Spatially-consistent Feature Matching and Learning for Heritage Image Analysis</papertitle>
              </a>
              </br>

              
              <strong>Xi Shen</strong>,
              <a href="https://robin-champenois.fr/">Robin Champenois</a>,
              <a href="https://people.eecs.berkeley.edu/~shiry/"> Shiry Ginosar</a>,
              <a href="http://www.chartes.psl.eu/en/ilaria-pastrolin">Ilaria Pastrolin</a>,
              <a href="https://www.ademec.com/en/association/members/morgane-rousselot">Morgane Rousselot</a>,
              <a href="https://scholar.google.com/citations?user=B2RS1M4AAAAJ&hl=en">Oumayma Bounou</a>,
              <a href="https://imagine.enpc.fr/~monniert/">Tom Monnier</a>,
              <a href="https://scholar.google.fr/citations?user=7atfg7EAAAAJ&hl=en">Spyros Gidaris</a>,
              <a href="https://www.irht.cnrs.fr/fr/annuaire/bougard-francois">Fran&ccedil;ois Bougard</a>,
              <a href="https://www.rocq.inria.fr/arles/index.php/software/106-pierre-guillaume-raverdy">Pierre-Guillaume Raverdy</a>,
              <a href="http://www.chartes.psl.eu/fr/marie-francoise-limon-bonnet">Marie-Fran&ccedil;oise Limon</a>,
              <a href="http://www.chartes.psl.eu/fr/christine-benevent">Christine B&eacute;n&eacute;vent</a>,
              <a href="http://www.chartes.psl.eu/fr/marc-smith">Marc Smith</a>,
              <a href="http://www.chartes.psl.eu/fr/olivier-poncet">Olivier Poncet</a>,
              <a href="https://scholar.google.be/citations?user=gW9pIqEAAAAJ&hl=en">K. Bender</a>,
              <a href="https://www.unige.ch/lettres/humanites-numeriques/fr/equipe/collaborateurs/prof-beatrice-joyeux-prunel/">B&eacute;atrice Joyeux-Prunel</a>,
              <a href="https://arthistory.berkeley.edu/people/elizabeth-honig/"> Elizabeth Honig</a>,
              <a href="https://people.eecs.berkeley.edu/~efros/"> Alexei A. Efros</a>,
            <a href="http://imagine.enpc.fr/~aubrym/">Mathieu Aubry</a>
              </br>
              <em>IJCV</em>, 2022
              </br>
              <a href="http://imagine.enpc.fr/~shenx/HisImgAnalysis/">[Project page]</a> &nbsp;
              <a href="https://link.springer.com/article/10.1007/s11263-022-01576-x">[PDF (Springer)]</a>
              &nbsp;
              <a href="https://hal.archives-ouvertes.fr/hal-03620996">[PDF (HAL)]</a>
            </td>
        </tr>


        <tr bgcolor="#ffffd0">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div style="display: flex; justify-content: center;">
                <img src='images/teaser_SegSwap.jpeg' width="160" >
              </div>
              
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="http://imagine.enpc.fr/~shenx/SegSwap/">
                <papertitle>Learning Co-segmentation by Segment Swapping for Retrieval and Discovery</papertitle>
              </a>
              </br>
              
              <strong>Xi Shen</strong>,
              <a href="https://people.eecs.berkeley.edu/~efros/"> Alexei A. Efros</a>,
              <a href="https://ai.facebook.com/people/armand-joulin/">Armand Joulin</a>,
              <a href="http://imagine.enpc.fr/~aubrym/">Mathieu Aubry</a>
              </br>
              <em>CVPR </em> <a href="https://image-matching-workshop.github.io/">Image Matching</a> workshop and <a href="https://sites.google.com/view/t4v-cvpr22">Transformer</a> workshop, 2022
              </br>
              <a href="https://github.com/EGO4D/ego-exo4d-relation/tree/main/correspondence/SegSwap">
              <span style="color: firebrick;">
              Segswap serves as the foundational baseline for Ego-Exo4D-Relation. 
              </span>
              </a>
              </br>
              
              <a href="http://imagine.enpc.fr/~shenx/SegSwap/">[Project page]</a> &nbsp;
              <a href="http://arxiv.org/abs/2110.15904">[PDF]</a> &nbsp;
              <a href="https://github.com/XiSHEN0220/SegSwap">[Code]</a> &nbsp;
              <a href="https://youtu.be/9pKwNGZPDr8">[Youtube Video]</a> &nbsp;
              <a href="http://imagine.enpc.fr/~shenx/SegSwap/slides.pdf">[Slides]</a> &nbsp;
              <a class="github-button" href="https://github.com/XiSHEN0220/SegSwap" data-icon="octicon-star" data-size="medium" data-show-count="true">Star</a>  
              

            </td>
        </tr>

        <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div style="display: flex; justify-content: center;">
                <img src='images/teaser_SSR.jpg' width="160" >
              </div>
              
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="http://imagine.enpc.fr/~shenx/SSR/">
                <papertitle>Re-ranking for image retrieval and transductive few-shot classification</papertitle>
              </a>
              </br>
              
              <strong>Xi Shen</strong>,
              <a href="https://youngxiao13.github.io/">Yang Xiao</a>,
            <a href="http://hushell.github.io/">Shell Xu Hu</a>,
            <a href="https://www.sbaiothman.com/">Othman Sbai</a>,
            <a href="http://imagine.enpc.fr/~aubrym/">Mathieu Aubry</a>
              </br>
              <em>NeurIPS</em>, 2021
              </br>
              <a href="http://imagine.enpc.fr/~shenx/SSR/">[Project page]</a> &nbsp;
              <a href="https://papers.nips.cc/paper/2021/file/d9fc0cdb67638d50f411432d0d41d0ba-Paper.pdf">[PDF]</a> &nbsp;
              <a href="https://github.com/XiSHEN0220/SSR">[Code]</a> &nbsp;
              <a href="http://imagine.enpc.fr/~shenx/SSR/ssr.mp4">[Video]</a> &nbsp;
              <a class="github-button" href="https://github.com/XiSHEN0220/SSR" data-icon="octicon-star" data-size="medium" data-show-count="true">Star</a>  
            </td>
        </tr>

        <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div style="display: flex; justify-content: center;">
                <img src='images/collation.jpeg' width="160" >
              </div>
              
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="http://imagine.enpc.fr/~shenx/ImageCollation/">
                <papertitle>Image Collation: Matching illustrations in manuscripts</papertitle>
              </a>
              </br>
              
            <a href="https://github.com/Rykoua">Ryad Kaoua</a>,
            <strong>Xi Shen</strong>,
            <a href="https://www.orient-mediterranee.com/spip.php?article487&lang=fr">Stavros Lazaris</a>,
            <a href="https://davidpicard.github.io/">David Picard</a>,
            <a href="http://imagine.enpc.fr/~aubrym/">Mathieu Aubry</a>
              </br>
              <em>ICDAR</em>, 2021
              </br>
              <a href="http://imagine.enpc.fr/~shenx/ImageCollation/">[Project page]</a> &nbsp;
              <a href="https://arxiv.org/abs/2108.08109">[PDF]</a> &nbsp;
              <a href="https://github.com/Rykoua/ImageCollation">[Code]</a> &nbsp;
              <a href="https://github.com/Rykoua/ImageCollation/tree/main/ManuscriptDownloader">[Dataset]</a> &nbsp;
              <a href="https://youtu.be/yXe7JHSJDUs">[Video]</a> &nbsp;
              <a href="http://imagine.enpc.fr/~shenx/ImageCollation/ICDAR_ImageCollation_Slides.pdf"> [Slides] </a> &nbsp;
              <a href="http://imagine.enpc.fr/~shenx/ImageCollation/icdar21.pdf"> [Poster] </a> &nbsp;
              <a class="github-button" href="https://github.com/Rykoua/ImageCollation" data-icon="octicon-star" data-size="medium" data-show-count="true">Star</a>  
              
            </td>
        </tr>

        <tr bgcolor="#ffffd0">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div style="display: flex; justify-content: center;">
                <img src='images/medici_small.gif' width="80" > &nbsp;
                <img src='images/RANSACflow_small.gif' width="80" >
                
              </div>
              
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="http://imagine.enpc.fr/~shenx/RANSAC-Flow/">
                <papertitle>RANSAC-Flow: generic two-stage image alignment</papertitle>
              </a>
              </br>
              
            <strong>Xi Shen</strong>,
            <a href="https://imagine-lab.enpc.fr/staff-members/francois-darmon">Fran&ccedil;ois  Darmon</a>,
            <a href="http://people.eecs.berkeley.edu/~efros/">Alexei A. Efros</a>,
            <a href="http://imagine.enpc.fr/~aubrym/">Mathieu Aubry</a>
              </br>
              <em>ECCV</em>, 2020
              </br>
              <a href="http://imagine.enpc.fr/~shenx/RANSAC-Flow/">[Project page]</a> &nbsp;
              <a href="https://arxiv.org/abs/2004.01526">[PDF]</a> &nbsp;
              <a href="https://github.com/XiSHEN0220/RANSAC-Flow">[Code]</a> &nbsp;
              <a href="https://youtu.be/ltZpqRtuA6A">[Demo]</a> &nbsp;
              <a href="https://youtu.be/FXqCZlmipdM">[Video]</a> &nbsp;
              <a href="http://imagine.enpc.fr/~shenx/RANSAC-Flow/slide_ransac_flow.pptx"> [Slides] </a> &nbsp;
              <a class="github-button" href="https://github.com/XiSHEN0220/RANSAC-Flow" data-icon="octicon-star" data-size="medium" data-show-count="true">Star</a>  
              
            </td>
        </tr>

        <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div style="display: flex; justify-content: center;">
                <img src='images/sib.jpeg' width="160" >
                
              </div>
              
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://github.com/hushell/sib_meta_learn">
                <papertitle>Empirical Bayes Transductive Meta-Learning with Synthetic Gradients</papertitle>
              </a>
              </br>
            
            <a href="http://hushell.github.io/">Shell Xu Hu</a>,
            <a href="https://www.linkedin.com/in/pablo-garc%C3%ADa-moreno-4b812087/?originalSubdomain=uk">Pablo Moreno</a>,
            <a href="https://youngxiao13.github.io/">Yang Xiao</a>,
            <strong>Xi Shen</strong>,
            <a href="http://imagine.enpc.fr/~obozinsg/">Guillaume Obozinski</a>,
            <a href="https://inverseprobability.com/biog">Neil D. Lawrence</a> 
            <a href="http://adamian.github.io/">Andreas Damianou</a>
              </br>
              <em>ICLR</em>, 2020
              </br>
              <a href="https://openreview.net/forum?id=Hkg-xgrYvH">[PDF]</a> &nbsp;
              <a href="https://github.com/hushell/sib_meta_learn">[Code]</a> &nbsp;
              <a class="github-button" href="https://github.com/hushell/sib_meta_learn" data-icon="octicon-star" data-size="medium" data-show-count="true">Star</a>  
              
            </td>
        </tr>

        <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div style="display: flex; justify-content: center;">
                <img src='images/watermark.png' width="160" > 
                
              </div>
              
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="http://imagine.enpc.fr/~shenx/Watermark/">
                <papertitle>Large-Scale Historical Watermark Recognition: dataset and a new consistency-based approach</papertitle>
              </a>
              </br>
              
            <strong>Xi Shen</strong>,
            <a href="http://www.chartes.psl.eu/en/ilaria-pastrolin">Ilaria Pastrolin</a>,
            <a href="https://scholar.google.com/citations?user=B2RS1M4AAAAJ&hl=en">Oumayma Bounou</a>,
            <a href="https://scholar.google.fr/citations?user=7atfg7EAAAAJ&hl=en">Spyros Gidaris</a>,
            <a href="http://www.chartes.psl.eu/fr/marc-smith">Marc Smith</a>,
            <a href="http://www.chartes.psl.eu/fr/olivier-poncet">Olivier Poncet</a>,
            <a href="http://imagine.enpc.fr/~aubrym/">Mathieu Aubry</a>
              </br>
              <em>ICPR</em>, 2020
              </br>
              <a href="http://imagine.enpc.fr/~shenx/Watermark/">[Project page]</a> &nbsp;
              <a href="http://arxiv.org/pdf/1908.10254.pdf">[PDF]</a> &nbsp;
              <a href="https://github.com/XiSHEN0220/WatermarkReco">[Code]</a> &nbsp;
              <a href="http://imagine.enpc.fr/~shenx/data/Watermark.zip">[Dataset]</a> &nbsp;
              <a href="https://www.youtube.com/embed/9Y47oyvjfQ8">[Video]</a> &nbsp;
              <a href="http://imagine.enpc.fr/~shenx/Watermark/watermarkReco.pptx">[Slides]</a> &nbsp;
              <a href="https://filigranes.inria.fr/#/filigrane-search">[Web App]</a>&nbsp;
              <a href="https://hal.archives-ouvertes.fr/hal-02513038/">[Paper on Web App]</a> &nbsp;
              <a class="github-button" href="https://github.com/XiSHEN0220/WatermarkReco" data-icon="octicon-star" data-size="medium" data-show-count="true">Star</a> 
              
              
            </td>
        </tr>


        <tr bgcolor="#ffffd0">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div style="display: flex; justify-content: center;">
                <img src='images/artminer.jpeg' width="160" >
                
              </div>
              
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="http://imagine.enpc.fr/~shenx/ArtMiner/">
                <papertitle>Discovering Visual Patterns in Art Collections with Spatially-consistent Feature Learning</papertitle>
              </a>
              </br>
              
            <strong>Xi Shen</strong>,
            <a href="http://people.eecs.berkeley.edu/~efros/">Alexei A. Efros</a>,
            <a href="http://imagine.enpc.fr/~aubrym/">Mathieu Aubry</a>
              </br>
              <em>CVPR</em>, 2019
              </br>
              <a href="https://www.nature.com/articles/d41586-019-01794-3">
              <span style="color: firebrick;">
              Covered by Nature.
              </span>
              </a>
              </br>
              
              <a href="http://imagine.enpc.fr/~shenx/ArtMiner/">[Project page]</a> &nbsp;
              <a href="https://arxiv.org/abs/1903.02678">[PDF]</a> &nbsp;
              <a href="https://github.com/XiSHEN0220/ArtMiner">[Code]</a> &nbsp;
              <a href="http://imagine.enpc.fr/~shenx/data/Brueghel.zip">[Dataset]</a> &nbsp;
              <a href="http://imagine.enpc.fr/~shenx/ArtMiner/artMiner.pdf"> [Slides] </a> &nbsp;
              <a class="github-button" href="https://github.com/XiSHEN0220/ArtMiner" data-icon="octicon-star" data-size="medium" data-show-count="true">Star</a> 
              
            </td>
        </tr>


        <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div style="display: flex; justify-content: center;">
                <img src='images/maan.png' width="160" >
                
              </div>
              
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://github.com/yyuanad/MAAN">
                <papertitle>Marginalized Average Attentional Network For Weakly-Supervised Learning</papertitle>
              </a>
              </br>
            
            <a href="https://yyuanad.github.io/">Yuan Yuan</a>, 
            <a href="https://yueminglyu.github.io/">Yueming Lyu</a>,   
            <strong>Xi Shen</strong>,
            <a href="https://www.uts.edu.au/staff/ivor.tsang">Ivor Tsang</a>,
            <a href="https://sites.google.com/view/dyyeung/">Dit-Yan Yeung</a>
              </br>
              <em>ICLR</em>, 2019
              </br>
              <a href="https://openreview.net/pdf?id=HkljioCcFQ">[PDF]</a> &nbsp;
              <a href="https://github.com/yyuanad/MAAN">[Code]</a> &nbsp;
              <a class="github-button" href="https://github.com/yyuanad/MAAN" data-icon="octicon-star" data-size="medium" data-show-count="true">Star</a> 
              
            </td>
        </tr>
    </tbody></table>



          
    

				
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>üéØ Misc.</heading>
              <hr/>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
          <h3> Academic Services </h3>
          <p>&nbsp; &nbsp; &nbsp; &nbsp; Reviewer for ICML, CVPR, ECCV, NeurIPS, ICLR, BMVC, WACV, TPAMI, PR, IJCV, ICCV. </p>
          <h3> Teacher Assistant </h3>
          <ul>
          <li> 
            <b style="background-color:#ddd;">03 / 2021 - 04 / 2021 &emsp; <a href="https://psl.eu/en"> Universit√© PSL</a> (Paris Sciences & Lettres)</b>&emsp;
            <a href="https://data-psl.github.io/intensive-week-dhai-2022/">Digital Humanities Meet Artificial Intelligence</a> (M1).
          </li>
          <li> 
            <b style="background-color:#ddd;">02 / 2021 - 06 / 2021 &emsp; <a href="https://www.ecoledesponts.fr">Ecole des Ponts ParisTech</a>  </b>&emsp;
            Computer Vision for Mechanics of materials (L3).
          </li>
          <li> <b style="background-color:#ddd;">02 / 2019 - 06 / 2019 &emsp; <a href="https://www.ecoledesponts.fr">Ecole des Ponts ParisTech</a>  </b>&emsp; Traitement de l'information et vision artificielle (M1).
          </li>
          
          </ul>

          <h3> Sports</h3>
          <p>&nbsp; &nbsp; &nbsp; &nbsp; I‚Äôm an amateur football player with a personal best of 11.30 seconds in the 100 meters. I also enjoy jogging, hiking, camping, and skiing. </p>

          <h3> Coding</h3>
          <p>&nbsp; &nbsp; &nbsp; &nbsp; All of my released code is maintained on my <a href="https://github.com/XiSHEN0220">GitHub account </a>.  </p>
          <table><tbody>
              
              
              <td width="10%"><p> <a class="github-button" href="https://github.com/XiSHEN0220/RANSAC-Flow" data-icon="octicon-star" data-size="large" data-show-count="true">Star</a></p></td>
              <td width="10%"><p> <a class="github-button" href="https://github.com/XiSHEN0220/RANSAC-Flow/fork" data-icon="octicon-repo-forked" data-size="large" data-show-count="true">Fork</a></p></td>
              <td width="20%">RANSAC-Flow</td>
             

             
             <td width="10%"><p> <a class="github-button" href="https://github.com/XiSHEN0220/ArtMiner" data-icon="octicon-star" data-size="large" data-show-count="true">Star</a></p></td>
              <td width="10%"><p> <a class="github-button" href="https://github.com/XiSHEN0220/ArtMiner/fork" data-icon="octicon-repo-forked" data-size="large" data-show-count="true">Fork</a></p></td>
              <td width="20%">ArtMiner</td>
             
        </tbody></table>
        <table><tbody>
             <td width="10%"><p> <a class="github-button" href="https://github.com/YangtaoWANG95/TokenCut" data-icon="octicon-star" data-size="large" data-show-count="true">Star</a></p></td>
              <td width="10%"><p> <a class="github-button" href="https://github.com/YangtaoWANG95/TokenCut/fork" data-icon="octicon-repo-forked" data-size="large" data-show-count="true">Fork</a></p></td>
              <td width="20%">TokenCut</td>


             

              <td width="10%"><p> <a class="github-button" href="https://github.com/hushell/sib_meta_learn" data-icon="octicon-star" data-size="large" data-show-count="true">Star</a></p></td>
              <td width="10%"><p> <a class="github-button" href="https://github.com/hushell/sib_meta_learn/fork" data-icon="octicon-repo-forked" data-size="large" data-show-count="true">Fork</a></p></td>
              <td width="20%">SIB</td>
        </tbody></table>
        <table><tbody>
             <td width="10%"><p> <a class="github-button" href="https://github.com/XiSHEN0220/SegSwap" data-icon="octicon-star" data-size="large" data-show-count="true">Star</a></p></td>
              <td width="10%"><p> <a class="github-button" href="https://github.com/XiSHEN0220/SegSwap/fork" data-icon="octicon-repo-forked" data-size="large" data-show-count="true">Fork</a></p></td>
              <td width="20%">SegSwap</td>

              <td width="10%"><p> <a class="github-button" href="https://github.com/XiSHEN0220/SSR" data-icon="octicon-star" data-size="large" data-show-count="true">Star</a></p></td>
              <td width="10%"><p> <a class="github-button" href="https://github.com/XiSHEN0220/SSR/fork" data-icon="octicon-repo-forked" data-size="large" data-show-count="true">Fork</a></p></td>
              <td width="20%">SSR</td>
        </tbody></table>

        <table><tbody>
             <td width="10%"><p> <a class="github-button" href="https://github.com/yingyichen-cyy/Nested-Co-teaching" data-icon="octicon-star" data-size="large" data-show-count="true">Star</a></p></td>
              <td width="10%"><p> <a class="github-button" href="https://github.com/yingyichen-cyy/Nested-Co-teaching/fork" data-icon="octicon-repo-forked" data-size="large" data-show-count="true">Fork</a></p></td>
              <td width="20%">NCT</td>

              <td width="10%"><p> <a class="github-button" href="https://github.com/XiSHEN0220/WatermarkReco" data-icon="octicon-star" data-size="large" data-show-count="true">Star</a></p></td>
              <td width="10%"><p> <a class="github-button" href="https://github.com/XiSHEN0220/WatermarkReco/fork" data-icon="octicon-repo-forked" data-size="large" data-show-count="true">Fork</a></p></td>
              <td width="20%">WatermarkReco</td>

              
        </tbody></table>

        <table><tbody>
             <td width="10%"><p> <a class="github-button" href="https://github.com/dulucas/siMLPe" data-icon="octicon-star" data-size="large" data-show-count="true">Star</a></p></td>
              <td width="10%"><p> <a class="github-button" href="https://github.com/dulucas/siMLPe/fork" data-icon="octicon-repo-forked" data-size="large" data-show-count="true">Fork</a></p></td>
              <td width="20%">siMLPe</td>

              <td width="10%"><p> <a class="github-button" href="https://github.com/Mael-zys/T2M-GPT" data-icon="octicon-star" data-size="large" data-show-count="true">Star</a></p></td>
              <td width="10%"><p> <a class="github-button" href="https://github.com/Mael-zys/T2M-GPT/fork" data-icon="octicon-repo-forked" data-size="large" data-show-count="true">Fork</a></p></td>
              <td width="20%">T2M-GPT</td>
              


              
        </tbody></table>
        

        </br>
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <hr/>
      <tr>
        <td>
        <p>
        <a href="https://www.easycounter.com/"><img src="https://www.easycounter.com/counter.php?shenxi"
            border="0" alt="Website Hit Counter"></a>
        <font size="2">unique visitors since Sep 2023</p>
        </td>

        <td>
        <p align="right"><font size="2">
          This website takes the template from <a href="https://github.com/jonbarron/website">here</a>.
        </font>

        </p>
        </td>
      </tr>
      </table>


        

          

          

          </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
